{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/thoang/workspace/spert/data/datasets/scierc/scierc_train.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinations\n",
      "strictly syntactic cross-serial agreement\n",
      "nouns\n",
      "reflexive pronouns\n",
      "grammatical number\n",
      "grammatical gender\n",
      "intrinsic object structure\n",
      "parameterized object state\n",
      "low dimensional manifold\n",
      "non-rigid part of object state\n",
      "intrinsic object structure\n",
      "complex non-rigid motions\n",
      "fish twisting\n",
      "self-occlusion\n",
      "inter-frame lip motion\n",
      "silent speech interface\n",
      "laryngectomees\n",
      "DSP\n",
      "DSP\n",
      "paraphrase\n",
      "paraphrases\n",
      "paraphrases\n",
      "paraphrases\n",
      "hand-produced sets\n",
      "paraphrase\n",
      "copying of unmodified subgraphs\n",
      "unmodified subgraphs\n",
      "log ( d ) overheads\n",
      "structure-sharing of graphs\n",
      "redundant copying\n",
      "quasi-destructive scheme 's ability\n",
      "over copying\n",
      "early copying\n",
      "cyclic structures\n",
      "subcategorization dictionary\n",
      "relative frequency of occurrence\n",
      "subcategorization dictionary\n",
      "prior knowledge\n",
      "low-level information\n",
      "noise\n",
      "partial occlusions\n",
      "missing image structure\n",
      "statistically learned prior knowledge\n",
      "joint intensity distribution\n",
      "joint intensity distributions\n",
      "intensity relations\n",
      "image modalities\n",
      "slice locations\n",
      "missing low-level information\n",
      "it\n",
      "intensity correspondences\n",
      "intensity distributions\n",
      "tiling\n",
      "tilings\n",
      "facade textures\n",
      "occluded parts inpainted\n",
      "facades\n",
      "inpainted parts\n",
      "multi-modalities\n",
      "image\n",
      "dense depth map\n",
      "complementary object information\n",
      "time consuming training stage\n",
      "untextured objects\n",
      "templates\n",
      "modalities\n",
      "commodity hardware\n",
      "single modalities\n",
      "image map\n",
      "dominant motion\n",
      "lack of temporal consistency\n",
      "temporal consistency\n",
      "validity of the dominant motion assumption\n",
      "dominant motion assumption\n",
      "oscillation between different scene interpretations and poor registration\n",
      "generic temporal constraint\n",
      "domain divergence\n",
      "viewpoints\n",
      "resolutions\n",
      "illuminations\n",
      "intermediate domain\n",
      "intermediate domain\n",
      "maximum margin criterion\n",
      "intra-class com-pactness\n",
      "inter-class penalty\n",
      "discriminative features\n",
      "aging faces\n",
      "dictionary bases\n",
      "aging process pattern\n",
      "personalized facial characteristics\n",
      "mole\n",
      "personality-aware coupled reconstruction loss\n",
      "personalized aging progression\n",
      "structure of communicative context\n",
      "desktop PCs\n",
      "deep link structures\n",
      "web visitor behavior\n",
      "linguistic information\n",
      "unification-based linguistic descriptions\n",
      "semantic and syntactic information\n",
      "deaf learners\n",
      "user proficiency\n",
      "constraint\n",
      "constraint\n",
      "short supports\n",
      "short support set\n",
      "full-length support set\n",
      "constraints\n",
      "short supports\n",
      "full-length supports\n",
      "drop-in replacement\n",
      "orthographic variants\n",
      "string similarity\n",
      "contextual similarity\n",
      "user initialization\n",
      "local detection uncertainties\n",
      "local detection uncertainties of multiple shape candidates\n",
      "shape alignment\n",
      "predicted shape prior\n",
      "subspace constraints\n",
      "maximum likelihood\n",
      "endocardium\n",
      "inter-expert variations\n",
      "syntax-based constraint\n",
      "cohesion constraint\n",
      "ranking function\n",
      "non-terminal symbol\n",
      "unknown words\n",
      "word n-grams\n",
      "positive markers\n",
      "automatic phonetic transcriptions ( APTs )\n",
      "manually verified phonetic transcriptions\n",
      "alignments\n",
      "APT\n",
      "MPT\n",
      "canonical transcription\n",
      "unknown transcriptions\n",
      "classification features\n",
      "fact assertions\n",
      "cognitive semantics\n",
      "Nonstationary chaotic behavior\n",
      "oxymoron\n",
      "nonstationary chaos\n",
      "nonstationary events\n",
      "alternative markers\n",
      "dialog\n",
      "operational semantics\n",
      "operational semantics\n",
      "hard decisions\n",
      "NLP structures\n",
      "hidden variables\n",
      "word clusters\n",
      "word senses\n",
      "hidden-variable assignments\n",
      "NLP structures\n",
      "parse trees\n",
      "shape motion\n",
      "rotation\n",
      "translation\n",
      "non-rigid deformation\n",
      "Reconstruction\n",
      "arbitrary deformations\n",
      "object shape\n",
      "3D shape and motion\n",
      "temporal smoothness in object shape\n",
      "positive feature vectors\n",
      "negative feature vectors\n",
      "generic , indicative summaries\n",
      "discourse segment\n",
      "user-directed summaries\n",
      "low-level features\n",
      "rich spatial-temporal structures\n",
      "high-level concepts\n",
      "action attributes\n",
      "action attributes\n",
      "data-driven attributes\n",
      "noisy and redundant attributes\n",
      "discriminative attributes\n",
      "compositional syntax rules\n",
      "post referencing\n",
      "intenslonal logic translator LILT\n",
      "derivational history\n",
      "reduced IL formulae\n",
      "stochastic gradients\n",
      "well defined objective function\n",
      "intolerable computational cost of BP\n",
      "segment order\n",
      "segmentation\n",
      "segment contiguity\n",
      "named entity annotations\n",
      "scenario templates\n",
      "unknown motions\n",
      "isolated 3D features\n",
      "surface\n",
      "model space\n",
      "regular-izer\n",
      "search space\n",
      "surfaces\n",
      "prior 2D-to-3D association\n",
      "face geometry\n",
      "search space\n",
      "morphological and syntactic information\n",
      "syntactic information\n",
      "morphology\n",
      "morphology\n",
      "word order\n",
      "free word order\n",
      "rich morphology\n",
      "monolingual and bilingual information\n",
      "recovered surface detail\n",
      "structured-light setup\n",
      "insufficient training data\n",
      "approximation error\n",
      "ambiguities\n",
      "separation margin\n",
      "trees\n",
      "forests\n",
      "syntactic information\n",
      "formal language\n",
      "epistemological objects\n",
      "epistemological level\n",
      "conceptual level\n",
      "relative rotation\n",
      "relative translation\n",
      "urban and indoor environments\n",
      "syntactic roles\n",
      "biological entities\n",
      "biomedical and linguistic ontologies\n",
      "syntactic roles\n",
      "multiple and nested interactions\n",
      "pattern engineering requirement\n",
      "rhetorical functions\n",
      "long-duration and complete occlusion\n",
      "prior knowledge\n",
      "shape\n",
      "motion of objects\n",
      "frame rate\n",
      "image size\n",
      "long-duration and complete occlusions\n",
      "changing background\n",
      "target vocabulary\n",
      "concept-based seeds\n",
      "NE\n",
      "PERSON NE\n",
      "decision list\n",
      "parsing-based NE rules\n",
      "predicate-argument structures\n",
      "features\n",
      "predicate-argument structures\n",
      "priori linguistic or phonetic knowledge\n",
      "human evaluators\n",
      "ASR output\n",
      "human transcription\n",
      "features\n",
      "lexical-cohesion and conversational features\n",
      "conversational cues\n",
      "cue phrases\n",
      "overlapping speech\n",
      "transcription errors\n",
      "ASR output\n",
      "lexical-cohesion and conversational features\n",
      "hubs\n",
      "automaton\n",
      "hub\n",
      "node\n",
      "graph\n",
      "minimal DFA\n",
      "hubs\n",
      "hubs\n",
      "root\n",
      "suffix\n",
      "conjugate priors\n",
      "conjugate prior\n",
      "conjugate prior\n",
      "Bregman divergence\n",
      "conjugate priors\n",
      "conjugate priors\n",
      "geometric understanding of conjugate priors\n",
      "conjugate priors\n",
      "non-terminal symbols\n",
      "latent variables\n",
      "Finegrained CFG rules\n",
      "averaging the statistics of independently trained models\n",
      "pooling of all the speech data\n",
      "directionality of dictionaries\n",
      "`` overlapping constraint ''\n",
      "dependency trees\n",
      "concepts\n",
      "coarse-grained concepts\n",
      "concept-concept matrix\n",
      "broad semantic classes\n",
      "English , Arabic and Spanish lexicons\n",
      "range concatenation languages [ RCL ]\n",
      "polynomial time\n",
      "O ( n6 ) time\n",
      "language L\n",
      "shared derivation forest\n",
      "AS\n",
      "structures of classes\n",
      "semi-Riemannian manifold\n",
      "submanifold\n",
      "ambient semi-Riemannian space\n",
      "class structures\n",
      "nullity of the semi-Riemannian space\n",
      "class structures\n",
      "feature space\n",
      "quadratic quantities of metric tensors\n",
      "semi-Riemannian space\n",
      "patterns\n",
      "rules\n",
      "linguistic rules\n",
      "control structures\n",
      "task-specific domain knowledge\n",
      "general linguistic knowledge\n",
      "grammatical and ungrammatical input\n",
      "case-frame instantiation\n",
      "conjunctions\n",
      "fragmentary input\n",
      "ungrammatical structures\n",
      "exotic , grammatically correct input\n",
      "ungrammatical input\n",
      "ASR errors\n",
      "smallest meaning-bearing units of language\n",
      "morphemes\n",
      "syntactic and semantic dependencies\n",
      "discourse structure\n",
      "discourse connectives\n",
      "inter-annotator variation\n",
      "temporal evolution\n",
      "intensity of the observed facial displays\n",
      "intrinsic topology of multidimensional continuous facial\n",
      "ordinal man-ifold\n",
      "H-CORF parameters\n",
      "ordinal manifold\n",
      "computer memory\n",
      "memory capacity\n",
      "random access\n",
      "disk\n",
      "memory size\n",
      "disk\n",
      "sense priors\n",
      "sense priors of words\n",
      "well calibrated probabilities\n",
      "well calibrated probabilities\n",
      "sense priors\n",
      "MWEs\n",
      "lexical units\n",
      "MWEs\n",
      "MWEs\n",
      "MWEs\n",
      "statistical parameters\n",
      "MWEs\n",
      "MWEs\n",
      "MWEs\n",
      "lexical units\n",
      "annotation\n",
      "semantic roles\n",
      "vagueness\n",
      "ambiguity\n",
      "new language pairs\n",
      "new domains\n",
      "affix removal\n",
      "rules\n",
      "context features\n",
      "full NPs\n",
      "referential elements\n",
      "ellipsis\n",
      "hierarchical relations\n",
      "superordinate - hyponym relation\n",
      "synonym relation\n",
      "features\n",
      "definition sentences\n",
      "hierarchical relations\n",
      "viewpoint\n",
      "illumination\n",
      "tensor space\n",
      "multiple factor interactions of training tensor\n",
      "modalities\n",
      "maximum likelihood identity parameter vector\n",
      "high-resolution tensor space\n",
      "imaging modalities\n",
      "model parameters\n",
      "blocks\n",
      "blocks\n",
      "unigram counts\n",
      "phrase length\n",
      "distribution\n",
      "features\n",
      "classifiers\n",
      "joint embed-dings of images and text\n",
      "multiple layers of linear projections\n",
      "nonlinearities\n",
      "large-margin objective\n",
      "cross-view ranking constraints\n",
      "within-view neighborhood structure preservation constraints\n",
      "metric learning literature\n",
      "translation relation\n",
      "isomorphic derivations\n",
      "translation relation\n",
      "bi-directional English-Spanish fragment\n",
      "rare behaviours of interest\n",
      "behaviours\n",
      "Global context\n",
      "Pragmatic aspects\n",
      "mutual information\n",
      "error-correction rules\n",
      "ambiguity of speech understanding\n",
      "user utterance\n",
      "statistical information\n",
      "hand-crafted rules\n",
      "IDF-weighted word overlap\n",
      "arguments\n",
      "defeat rules\n",
      "defeasibility\n",
      "labial coarticulation effects\n",
      "selection of articulatory targets\n",
      "failed parses\n",
      "world knowledge\n",
      "names\n",
      "times and numerical quantities\n",
      "deterministic internal feature of the words\n",
      "capitalization\n",
      "digitalization\n",
      "internal gazetteer feature\n",
      "external macro context feature\n",
      "handcrafted rules\n",
      "documents with signal content\n",
      "subjective visual attribute\n",
      "interesting-ness value\n",
      "pairwise comparisons\n",
      "annotation outliers/errors\n",
      "annotation outliers\n",
      "tight NEx-PTlME complexity bounds\n",
      "overfitting\n",
      "neighborhood graph structure\n",
      "known distribution\n",
      "Spatial , temporal and periodic information\n",
      "manifold\n",
      "search space\n",
      "edge\n",
      "chart\n",
      "edges\n",
      "edges\n",
      "spanning edges\n",
      "edges\n",
      "function words\n",
      "unknown words\n",
      "reduction in the search space\n",
      "semantic\n",
      "syntactic categories\n",
      "terminal and non-terminal edges\n",
      "edges\n",
      "edges\n",
      "word significance\n",
      "significance\n",
      "word significance ( weights )\n",
      "languages\n",
      "English\n",
      "agglutinative languages\n",
      "agglutinative languages\n",
      "two-level morphology\n",
      "Turkish\n",
      "maximum likelihood criterion\n",
      "real-valued features\n",
      "language model score\n",
      "binary features\n",
      "features\n",
      "data structure\n",
      "memory\n",
      "phrase translations\n",
      "suffix array-based data structure\n",
      "similarity between words\n",
      "word vectors\n",
      "taxonomic similarity\n",
      "associative similarity\n",
      "dictionary-based word vectors\n",
      "taxonomic similarity\n",
      "LSA-based and the cooccurrence-based word vectors\n",
      "associative similarity\n",
      "machine translation tests\n",
      "human annotation\n",
      "dependency tree\n",
      "right-side dependencies\n",
      "left-side dependents\n",
      "right-side nominal dependents\n",
      "right-side verbal dependents\n",
      "typing location\n",
      "commands\n",
      "rules\n",
      "mathematical expressions\n",
      "paraphrases\n",
      "latent variable\n",
      "paraphrase\n",
      "topic information\n",
      "N-Best sentence hypotheses\n",
      "grammar coverage problems\n",
      "board\n",
      "Intel i860 chip\n",
      "SUN 4\n",
      "straight C code\n",
      "board\n",
      "VME bus\n",
      "SUN4\n",
      "application back end\n",
      "features\n",
      "geometric prior\n",
      "image appearance space\n",
      "location grouping space\n",
      "parallel transport on manifolds\n",
      "heterogeneous data modalities\n",
      "geo-tags\n",
      "videos\n",
      "weak duration constraints\n",
      "word matches\n",
      "unrealistic durations\n",
      "word duration constraints\n",
      "lattice\n",
      "word duration probabilities\n",
      "state transitions\n",
      "duration constraints\n",
      "word matches\n",
      "noise conditions\n",
      "presuppositional nature\n",
      "extended domain of locality ( EDOL )\n",
      "EDOL\n",
      "citations\n",
      "publication\n",
      "mappings\n",
      "domain\n",
      "object characteristics\n",
      "author names\n",
      "citations\n",
      "probabilistic decision tree\n",
      "probabilistic decision trees\n",
      "translation equivalences\n",
      "parallel concordances\n",
      "translation lexicon\n",
      "emoticons\n",
      "dynamic scene\n",
      "geometric classes\n",
      "appearance and motion features\n",
      "segmentation hierarchy levels\n",
      "granularity a priori\n",
      "geometric context of video\n",
      "annotated outdoor videos\n",
      "high confidence predictions\n",
      "unlabeled data\n",
      "geometric context of video\n",
      "linguistic and non-linguistic knowledge\n",
      "natural language\n",
      "natural language input\n",
      "parsing flexibilities\n",
      "restricted natural language\n",
      "activity\n",
      "discussing\n",
      "planning\n",
      "informing\n",
      "story-telling\n",
      "dominance distribution of speakers\n",
      "dictionary word sense definitions\n",
      "restricted vocabulary\n",
      "word sense definitions\n",
      "restricted vocabulary\n",
      "phrasal analysis rules\n",
      "incomplete knowledge of phrasal constructions\n",
      "segmentation bakeoff\n",
      "PK-open\n",
      "PK-closed\n",
      "AS-open\n",
      "AS-closed\n",
      "HK-open\n",
      "HK-closed\n",
      "MSR-open\n",
      "MSR - closed\n",
      "MSR-open\n",
      "MSR-close\n",
      "PK-open\n",
      "semantic relations\n",
      "synonymy\n",
      "antonymy\n",
      "hyponymy\n",
      "meronymy\n",
      "causal and troponymic entailment\n",
      "semantically related words\n",
      "alternative senses\n",
      "polysemous word\n",
      "polysemous word\n",
      "polysemous word\n",
      "semantic distance\n",
      "discourse segments\n",
      "display of graphical information\n",
      "graphical information\n",
      "nouns\n",
      "SENSEVAL-2 nouns\n",
      "domain dependence\n",
      "quantification over events\n",
      "temporal connective\n",
      "temporal connective\n",
      "subordinate clause\n",
      "proportion problem\n",
      "reference time\n",
      "correlation of dependency relation paths\n",
      "dependency relations\n",
      "mapping score\n",
      "natural language ( NL ) interfaces\n",
      "speech or text input\n",
      "single depth sensor\n",
      "human template\n",
      "global consistency constraints\n",
      "anim-itable avatar\n",
      "interference\n",
      "GNSS measurement noise\n",
      "covariance\n",
      "EKF outputs\n",
      "variance jumps\n",
      "GNSS signal\n",
      "convex constraints\n",
      "regularization\n",
      "occlusions\n",
      "object boundaries\n",
      "motion segments\n",
      "poorly-textured surfaces\n",
      "shape template\n",
      "poorly-textured surfaces\n",
      "poorly-textured , deformable surface\n",
      "hypergraph regu-larization\n",
      "hypergraph\n",
      "hypergraph regularization\n",
      "outliers\n",
      "outliers\n",
      "feature-based partial descriptions\n",
      "flexible betting language\n",
      "overfitting\n",
      "general translation lexicons\n",
      "domain-specific translation lexicons\n",
      "policies\n",
      "Markov decision process\n",
      "policy\n",
      "real-life trials\n",
      "real-life trials\n",
      "real-life trials\n",
      "near-optimal performance\n",
      "real-life trials\n",
      "near-optimal performance\n",
      "computational lexicon\n",
      "computational lexicon\n",
      "shared lexical information\n",
      "syntactic structure\n",
      "speech acts\n",
      "sentence plans\n",
      "text-plan input\n",
      "sentence plans\n",
      "ranking rules\n",
      "sentence plan\n",
      "top human-ranked sentence plan\n",
      "reestimation formulas\n",
      "word-based collocational properties\n",
      "qualitative temporal and topological relations\n",
      "qualitative information\n",
      "partial consistency\n",
      "◆ G-consistency\n",
      "patchwork property\n",
      "derived words\n",
      "two-level morphology\n",
      "hierarchical lexicon\n",
      "Polymorphemic stems\n",
      "compositional interpretation\n",
      "derived words\n",
      "words formed ad-hoc\n",
      "sentiment\n",
      "Negations\n",
      "modifiers\n",
      "sentiment keywords\n",
      "sentiment\n",
      "scope ambiguity\n",
      "dominance graphs\n",
      "degree of ambiguity\n",
      "LFG\n",
      "PATR-II\n",
      "rules\n",
      "hierarchical ( or other types of ) relations\n",
      "morphological variants\n",
      "lexically-induced relations\n",
      "MeSH relations\n",
      "MeSH\n",
      "MeSH\n",
      "complex concepts\n",
      "relationships\n",
      "events\n",
      "structure of a diagram\n",
      "structure of diagrams\n",
      "illuminations\n",
      "normal-aware lighting difference\n",
      "camera geometry correction flow\n",
      "real scene change mask\n",
      "coarse-to-fine manner\n",
      "change decision\n",
      "varied multiple lighting conditions\n",
      "real scene changes\n",
      "lighting variations\n",
      "language pairs\n",
      "English-Chinese\n",
      "English-Japanese\n",
      "character level\n",
      "character level\n",
      "spectrum differential\n",
      "voice timbre\n",
      "converted singing voice waveforms\n",
      "singing voice characteristics\n",
      "converted singing voice\n",
      "natural singing voice\n",
      "spectra\n",
      "differential spectral feature\n",
      "fixed-length feature vectors\n",
      "low-dimensional space ( i-vectors )\n",
      "audio processing field\n",
      "strict compositionality\n",
      "Mimo\n",
      "linguistic phenomena\n",
      "wh-movement\n",
      "the passive and the binding of reflexives and pronouns\n",
      "wh-movement\n",
      "local regions\n",
      "background clutter\n",
      "occlusion\n",
      "vocabulary tree\n",
      "vocabulary tree\n",
      "tree\n",
      "quantization\n",
      "quantization\n",
      "indexing\n",
      "reverberant enclosures\n",
      "reverberant environments\n",
      "reverberation time\n",
      "reverberated speech utterance\n",
      "artificial re-verberant conditions\n",
      "simulated and real reverberant environments\n",
      "Dynamic Hierarchical Phrasal Lexicon ( DHPL )\n",
      "lexical hierarchy\n",
      "linguistic concepts\n",
      "lexical hierarchy\n",
      "linguistic concepts\n",
      "German pronouns\n",
      "pronouns\n",
      "subcategorization cues\n",
      "punctuation\n",
      "punctuation\n",
      "subcategorization cues\n",
      "punctuation\n",
      "pos tag ambiguity\n",
      "fine-grained grammatical categories\n",
      "tag\n",
      "ccg\n",
      "premature ambiguity resolution\n",
      "lexical category ambiguity\n",
      "pos level\n",
      "pos tags\n",
      "pos tag ambiguity\n",
      "physical hypotheses\n",
      "simple image regions\n",
      "coherent surfaces\n",
      "regions of similar color\n",
      "segmentations\n",
      "coherent surfaces\n",
      "semantic and pragmatic level\n",
      "SmartKom\n",
      "virtual communication assistant\n",
      "graphical display\n",
      "XML-based markup language\n",
      "illumination-based constraints\n",
      "occluded image regions\n",
      "inpainting\n",
      "highlight pixels\n",
      "Constraints\n",
      "pixel colors\n",
      "highlight color analysis\n",
      "illumination color uniformity\n",
      "estimation of the underlying diffuse color\n",
      "illumination constraints\n",
      "recovery of shading and textures\n",
      "objective function\n",
      "lo-calization constraint\n",
      "non-negativity constraint\n",
      "localized features\n",
      "ontologies\n",
      "rules\n",
      "errors\n",
      "fixes\n",
      "discourse relations\n",
      "syntactically motivated relations in discourse\n",
      "syntactico-semantic ( tectogrammatical ) annotation\n",
      "informative frames\n",
      "geometric structure of crowd patterns\n",
      "layers\n",
      "homographies\n",
      "planar patches\n",
      "scene\n",
      "low dimensional linear subspace\n",
      "Layers\n",
      "subspace\n",
      "clusters\n",
      "regions\n",
      "noise\n",
      "subspace constraint\n",
      "constraints\n",
      "graph structure\n",
      "probability distributions\n",
      "graph\n",
      "conditional inde-pendencies\n",
      "algebraic constraints\n",
      "conditional independencies\n",
      "Verma constraints\n",
      "Verma constraints\n",
      "dormant independencies\n",
      "conditional independencies\n",
      "interventional distributions\n",
      "dormant independence\n",
      "variables\n",
      "causal graph\n",
      "independence\n",
      "interventional distribution\n",
      "interventions\n",
      "dormant independencies\n",
      "dormant independencies\n",
      "extraneous edges\n",
      "causal graph\n",
      "statistical analyses\n",
      "triplets of words\n",
      "sentence co-occurrences\n",
      "features\n",
      "ungrammatical input\n",
      "rotational speed of the engine\n",
      "vibrations\n",
      "electrical system voltage level\n",
      "ambient sound\n",
      "speed\n",
      "gears\n",
      "gear scale factors\n",
      "speed measurements\n",
      "bit rates\n",
      "buzzy or metallic artefacts\n",
      "excitation source\n",
      "low bit rates\n",
      "LPC excitation\n",
      "frequency bands\n",
      "variable cutoff frequency\n",
      "low bit rates\n",
      "instantaneous linear mixtures\n",
      "mixing matrix\n",
      "sparsity of sources\n",
      "signal dictionary\n",
      "multi scale transforms\n",
      "wavelet or wavelet packets\n",
      "local features\n",
      "sparsity\n",
      "features\n",
      "bottleneck features\n",
      "typed feature structures\n",
      "top-down derivation\n",
      "disjunctive feature structures\n",
      "derivation tree\n",
      "speaker 's intention\n",
      "telephone dialogue\n",
      "keywords\n",
      "image semantics\n",
      "visual vocabulary\n",
      "blob-tokens\n",
      "image content\n",
      "blob-tokens\n",
      "keywords\n",
      "keywords\n",
      "blob-token set\n",
      "temporal information\n",
      "temporal expressions\n",
      "temporal expressions in newswire texts\n",
      "temporal expressions\n",
      "constraint-based representation of time\n",
      "Time Calculus for Natural Language ( TCNL )\n",
      "camera viewpoint\n",
      "rigid structure-from-motion\n",
      "object shapes\n",
      "visual hull proposals\n",
      "loose within-class shape similarity assumptions\n",
      "projection cone\n",
      "positions\n",
      "velocities\n",
      "appearance\n",
      "translation invariance\n",
      "positions\n",
      "velocities\n",
      "appearance\n",
      "global variables\n",
      "global properties\n",
      "translation\n",
      "scale\n",
      "viewpoint\n",
      "global variables\n",
      "translation\n",
      "local variables\n",
      "relative positions\n",
      "appearances of body parts\n",
      "occlusions\n",
      "linear relationships between observed variables\n",
      "lower-dimensional hidden space\n",
      "observed variables\n",
      "linear combination of products of normally distributed hidden variables\n",
      "distributed hidden variables\n",
      "distributed hidden variables\n",
      "hidden space\n",
      "natural language interface\n",
      "text-to-speech form\n",
      "Deictic reference\n",
      "feedback\n",
      "discourse\n",
      "syntax\n",
      "semantics\n",
      "propositional language of context\n",
      "propositional logic of context\n",
      "classical propositional logic\n",
      "modality\n",
      "correspondence theory\n",
      "ultra-wide baselines\n",
      "large camera rotations\n",
      "appearance variation\n",
      "local correspondence\n",
      "local correspondences\n",
      "grammar sensu stricto\n",
      "fine-grained annotation\n",
      "paraphrases\n",
      "phrasal and single word lexical paraphrases\n",
      "syntactic paraphrases\n",
      "blurred regions\n",
      "blur features\n",
      "image color\n",
      "gradient\n",
      "spectrum information\n",
      "image patches\n",
      "blur information\n",
      "polysemous words\n",
      "discourse\n",
      "polysemous word\n",
      "well-written discourse\n",
      "discourse\n",
      "constraint\n",
      "discourse constraint\n",
      "homographies\n",
      "higher-dimensional real or complex space\n",
      "homography\n",
      "complex bilinear form\n",
      "real quadratic form\n",
      "homo-graphies\n",
      "piece-wise planar scene\n",
      "homogeneous regions of foreground objects\n",
      "common language ( US English )\n",
      "microphone vs. telephone channel\n",
      "continuous speech\n",
      "isolated words\n",
      "mismatch\n",
      "rules\n",
      "unambiguous structures\n",
      "argumental relations\n",
      "modifier attachment\n",
      "global parse tree\n",
      "information graphic\n",
      "sight-impaired users\n",
      "information graphics\n",
      "BN model parameters\n",
      "qualitative prior knowledge\n",
      "domain experts\n",
      "physical or geometric constraints\n",
      "quantitative prior\n",
      "qualitative prior\n",
      "qualitative knowledge\n",
      "generic qualitative constraints\n",
      "BN model parameters\n",
      "constraints on trees\n",
      "constraints on trees decorated with feature structures\n",
      "polysemic verbs\n",
      "polysemy\n",
      "clusters\n",
      "pan-tilt-zoom ( PTZ ) cameras\n",
      "panoramic area\n",
      "high resolution imagery\n",
      "PTZ cameras\n",
      "prior knowledge of intrinsic parameters of the PTZ camera\n",
      "relative positioning\n",
      "orientation\n",
      "PTZ cameras\n",
      "relative positioning\n",
      "orientation\n",
      "PTZ cameras\n",
      "PTZ camera\n",
      "relative positions\n",
      "PTZ cameras\n",
      "words subset\n",
      "uniform color and texture properties\n",
      "codebook of region types\n",
      "spatial relationships\n",
      "OCR system\n",
      "output of black-box OCR systems\n",
      "OCR systems\n",
      "feature functions\n",
      "syntactic information\n",
      "IBM Model 3 alignment probabilities\n",
      "POS correspondence\n",
      "bilingual dictionary coverage\n",
      "features\n",
      "geometric transformation space\n",
      "feature detection errors\n",
      "inflexible quan-tization of single feature correspondences\n",
      "multiple dithered transformations\n",
      "transformation quantization\n",
      "regards mismatches\n",
      "geometric constraints\n",
      "dithering process\n",
      "non-uniformity\n",
      "spatial similarity\n",
      "multiple matching surfaces\n",
      "fluorescence\n",
      "Fluorescence\n",
      "natural gems\n",
      "fluorescent dyes\n",
      "fluorescence\n",
      "fluorescent materials\n",
      "fluo-rescence\n",
      "similarity\n",
      "fluorescence\n",
      "diffuse reflection\n",
      "fluorescence\n",
      "fluorescence 's wavelength-shifting property\n",
      "shape\n",
      "specular reflection\n",
      "reflection\n",
      "supervision from semantic labels\n",
      "temporal order\n",
      "semantic labels\n",
      "complementary information\n",
      "human pose\n",
      "human pose\n",
      "supervision\n",
      "Our method\n",
      "multisentential discourse plans\n",
      "discourse plans\n",
      "multisentential and multi-paragraph explanations\n",
      "cognito-pragmatic nature\n",
      "discourse\n",
      "knowledge store\n",
      "determination process\n",
      "social and cognitive psychology\n",
      "FDA\n",
      "discourse-level construct\n",
      "model parameters\n",
      "partition function\n",
      "generalized metaphor\n",
      "bilingual dependency relations\n",
      "monolingual dependency parse\n",
      "bilingual context of dependency relation\n",
      "paraphrases\n",
      "generalized translation knowledge\n",
      "paraphrases\n",
      "generalized translation knowledge\n",
      "paraphrases\n",
      "translation knowledge\n",
      "paraphrases\n",
      "Montague semantics\n",
      "resource sensitive logic\n",
      "Montague semantics\n",
      "formal computation of the logical form\n",
      "logical form\n",
      "core English lexicon\n",
      "news reports\n",
      "event\n",
      "conceptual mappings\n",
      "discourse patterns\n",
      "word graph\n",
      "cepstrum-based features\n",
      "co-occurrence pattern\n",
      "binary or local features\n",
      "co-occurrence patterns\n",
      "conjunction ( AND ) and disjunction ( OR ) of binary features\n",
      "discriminative co-occurrence patterns\n",
      "optimal co-occurrence pattern\n",
      "minimum empirical error\n",
      "AND and OR patterns\n",
      "dis-criminative co-occurrence patterns\n",
      "Occam 's Razor argument\n",
      "ambiguity\n",
      "ambiguity\n",
      "French tenses\n",
      "theory of tenses\n",
      "meaning of the tenses\n",
      "event structure\n",
      "preceeding text\n",
      "temporal adverbials\n",
      "reference times\n",
      "temporal perspective times\n",
      "speech time\n",
      "location time\n",
      "meaning of the tenses\n",
      "features\n",
      "alignment errors in multilingually aligned wordnets\n",
      "3D geometrical configuration\n",
      "3D points and cameras\n",
      "image coordinates\n",
      "rational quartic curve\n",
      "idiosyncracies of the new sublanguage\n",
      "feature structures\n",
      "direct mappings of one feature structure into another\n",
      "inverse Hessian matrix\n",
      "Hessian\n",
      "randomly generated compressed form of the Hessian\n",
      "batch stochastic gradients\n",
      "linear convergence\n",
      "dictionaries of word forms\n",
      "inflection\n",
      "inflection\n",
      "lexical probabilities\n",
      "linear convergence\n",
      "data dependent local smoothness\n",
      "loss functions\n",
      "optimum\n",
      "convergence guarantees\n",
      "local smoothness\n",
      "local smoothness\n",
      "conditioning variables\n",
      "morphological or syntactic features\n",
      "model parameters\n",
      "cooperative responses\n",
      "cooperative responses\n",
      "dialogue duration\n",
      "layout structures\n",
      "linguistic pattern\n",
      "paraphrases\n",
      "paraphrases\n",
      "paraphrase probability\n",
      "paraphrases\n",
      "translation probabilities\n",
      "contextual information\n",
      "paraphrases\n",
      "automatic alignments\n",
      "answer expressions\n",
      "syntax of answer expressions\n",
      "CAS\n",
      "associative relationships between verb phrases\n",
      "associative relationship\n",
      "associative relationship\n",
      "scenario consistency\n",
      "out-of-vocabulary keywords\n",
      "reduction of out-of-vocabulary items\n",
      "language functionalities\n",
      "referring expressions\n",
      "lexical choice\n",
      "revision\n",
      "discourse markers\n",
      "dictionary\n",
      "candidate parses\n",
      "ranking\n",
      "ranking\n",
      "features\n",
      "tree\n",
      "tree\n",
      "features\n",
      "features\n",
      "derivation\n",
      "features\n",
      "log-likelihood\n",
      "features\n",
      "parse trees\n",
      "sparsity of the feature space\n",
      "class of languages\n",
      "reduplication\n",
      "reduplication\n",
      "stack\n",
      "class of languages\n",
      "reduplications\n",
      "stopping criterion\n",
      "probability distribution\n",
      "clusters\n",
      "proximity matrix\n",
      "local subspace structure\n",
      "Initial clusters\n",
      "global data structure\n",
      "finer clusters\n",
      "subtle class differences\n",
      "global scale\n",
      "Euclidean space\n",
      "computational problems\n",
      "exponential time lower-bound\n",
      "pri-ori knowledge\n",
      "linearity\n",
      "non-linearities\n",
      "rotations\n",
      "user interaction\n",
      "modes of variations\n",
      "graph of translation hypotheses\n",
      "hypotheses graph\n",
      "nodes\n",
      "vectors representing morpho-syntactic properties\n",
      "statistical feature functions\n",
      "feature functions\n",
      "log-linear combination\n",
      "translation paths\n",
      "graph\n",
      "PoS-tag feature function\n",
      "coherence rules\n",
      "internal and contextual information\n",
      "domain specific terms\n",
      "features\n",
      "features\n",
      "delimiters\n",
      "term frequency\n",
      "hard rules\n",
      "domain knowledge\n",
      "systematic patterns in translation data\n",
      "patterns in machine translation output\n",
      "number of hidden layers\n",
      "threshold units\n",
      "hidden layer\n",
      "hidden layer\n",
      "global computability\n",
      "hidden layer\n",
      "non-local configuration\n",
      "`` critical cycle ''\n",
      "hidden layer\n",
      "unknown illumination conditions\n",
      "intrinsic textures\n",
      "pixel-resolution surface textures\n",
      "intrinsic appearance parameters\n",
      "uniform albedo\n",
      "global lighting reconstruction\n",
      "texture and coarse scene geometry\n",
      "inherent global ambiguity in shading\n",
      "multiple view video capture\n",
      "reproduction of fine surface detail\n",
      "rankings\n",
      "official rankings\n",
      "system response\n",
      "speech cues\n",
      "noise\n",
      "reverberation\n",
      "additive noise\n",
      "reverberation\n",
      "noise\n",
      "reverberation\n",
      "higher-order cepstral coefficients\n",
      "cross-corpus ( and cross-language ) experiment\n",
      "Sentence ambiguities\n",
      "domain targeted preference knowledge\n",
      "dependency structures\n",
      "interfaces\n",
      "speech knowledge\n",
      "features\n",
      "subcategorization frames\n",
      "subcategorization frames\n",
      "non-linear nature\n",
      "surveillance scenarios\n",
      "gravity vector\n",
      "nonlinear monomials\n",
      "embedded hardware\n",
      "compacted motion features\n",
      "motion information\n",
      "rich motion information\n",
      "low dimension feature vector\n",
      "expanded stop lists\n",
      "Frontal views of human faces\n",
      "gender judgment\n",
      "reaction time\n",
      "confidence rating\n",
      "Principal Components of the texture\n",
      "hyperplane of the learning algorithms\n",
      "feature space\n",
      "hyperplane\n",
      "linguistic information\n",
      "ambiguous wordform\n",
      "inflected forms\n",
      "ambiguous word\n",
      "hierarchy\n",
      "coarser graphs\n",
      "finest graph\n",
      "energy function\n",
      "spatio-temporal continuity\n",
      "question , passage , and/or answer levels\n",
      "sublanguage\n",
      "unknown words\n",
      "personal names\n",
      "lexical substitutions\n",
      "pronominalization\n",
      "superordinate substitution\n",
      "definite noun phrase reiteration\n",
      "lexical substitutions\n",
      "positions of topic-bearing sentences\n",
      "genre-specific regularities of discourse structure\n",
      "notion of hypothesis sharing\n",
      "shallow linguistic features of questions\n",
      "user 's informational goals\n",
      "training and testing factors\n",
      "linguistic structures\n",
      "linguistic information\n",
      "lexical , syntactic , semantic , and structural information\n",
      "parse tree\n",
      "parse tree\n",
      "parse\n",
      "parse\n",
      "head-to-head tests\n",
      "verb forms\n",
      "deictic information\n",
      "aspectual information\n",
      "model-theoretic semantics\n",
      "word-sense distinctions\n",
      "word-sense distinctions\n",
      "verb semantics\n",
      "syntactic behavior\n",
      "semantic information\n",
      "syntactic cues\n",
      "syntactic cues\n",
      "word senses\n",
      "word senses\n",
      "user interfaces\n",
      "user interface styles or conventions\n",
      "TIPSTER Architecture specification\n",
      "Graphical User Interface ( GUI ) functions\n",
      "GUIs\n",
      "multilingual TIPSTER user interfaces\n",
      "natural language interfaces\n",
      "natural language interfaces\n",
      "personal computers\n",
      "graphics displays\n",
      "natural language interfaces\n",
      "flow field\n",
      "moving crowd\n",
      "grid of particles\n",
      "flow field\n",
      "evolution of particles\n",
      "spatial gradients\n",
      "maximum eigenvalue\n",
      "Finite Time Lyapunov Exponent ( FTLE ) field\n",
      "Lagrangian Coherent Structures ( LCS )\n",
      "LCS\n",
      "boundaries of the flow segments\n",
      "block reordering\n",
      "edit operation\n",
      "quadratic time\n",
      "word-dependent substitution costs\n",
      "human judgment\n",
      "word dependent substitution costs\n",
      "human judgment\n",
      "Assertions\n",
      "variables\n",
      "universal quantification\n",
      "rule\n",
      "dialogue patterns\n",
      "small-sample-size\n",
      "higher dimensional ( kernel ) subspaces\n",
      "discrimination ability\n",
      "semantic equivalence\n",
      "asymmetric , or directional , relations\n",
      "local structure of coherent text\n",
      "discourse relations\n",
      "verb entailment types\n",
      "highly varied argument structures\n",
      "point clouds\n",
      "static Schrödinger equation\n",
      "static Hamilton-Jacobi equation\n",
      "analytic expression\n",
      "theoretical physics literature\n",
      "square-root density\n",
      "unit Hilbert sphere\n",
      "intrinsic geometry\n",
      "space of densities\n",
      "analytic expressions\n",
      "geodesic distance\n",
      "rigid and non-rigid transformations\n",
      "High frequency oscillations ( HFOs )\n",
      "epileptic brain tissue and activity\n",
      "HFOs\n",
      "features space\n",
      "linear manifold\n",
      "manifold\n",
      "bounds\n",
      "Bayes classification error\n",
      "HFOs\n",
      "HFO features\n",
      "discrete events\n",
      "action potentials\n",
      "multi-unit activity\n",
      "ontology\n",
      "speech recognition hypotheses ( SRH )\n",
      "semantic coherence\n",
      "speech recognition hypotheses\n",
      "SRHs\n",
      "N-best candidates of ASR\n",
      "contextual information\n",
      "correct information presentation\n",
      "redundant turns\n",
      "question-answering capability\n",
      "information access\n",
      "memory constraints .\n",
      "rank\n",
      "entropy\n",
      "pruning criteria\n",
      "rank\n",
      "rank\n",
      "respectful words\n",
      "referential information\n",
      "zero pronouns\n",
      "machine translation outputs\n",
      "ranks\n",
      "ranks\n",
      "Visually-guided arm reaching movements\n",
      "coordinated action of neu-rons\n",
      "neuronal population vector ( NPV )\n",
      "NPV\n",
      "arm posture\n",
      "cortical motor command\n",
      "NPV\n",
      "motor cortex\n",
      "broadly-tuned ( muscular ) proprioceptive\n",
      "( cartesian ) visual information\n",
      "( angular ) motor commands\n",
      "two-link arm\n",
      "NPV\n",
      "NPV\n",
      "image of cortical processing\n",
      "arm reaching movements\n",
      "pairwise constraints\n",
      "sampled pairwise constraints\n",
      "projections\n",
      "pairwise constraints\n",
      "supervision\n",
      "object category\n",
      "visual words\n",
      "spatial information\n",
      "intra-class variability\n",
      "unrelated images\n",
      "analytical expressions\n",
      "Markov chains\n",
      "learning curve behavior\n",
      "step-size and eligibility trace parameters\n",
      "Italian and English verb subcategorization frames ( SCFs )\n",
      "lexico-syntactic knowledge\n",
      "SCFs\n",
      "SCFs distributions\n",
      "similar semantic properties\n",
      "multi-core machines\n",
      "synchronization overhead\n",
      "parallelism\n",
      "graph\n",
      "regularly-shaped dis-joint subgraphs\n",
      "subgraphs\n",
      "global optimum\n",
      "subgraphs\n",
      "balanced workloads\n",
      "overhead\n",
      "evaluation function\n",
      "Expected-IoU ( EIoU ) score\n",
      "Expected-Intersection-over-Expected-Union ( EIoEU )\n",
      "EIoU\n",
      "EIoEU\n",
      "margins distribution\n",
      "lexical rules\n",
      "multi-view video observations\n",
      "reduced drift\n",
      "non-rigid deformations\n",
      "branches\n",
      "tree\n",
      "error accumulation\n",
      "local segments\n",
      "global non-sequential traversal\n",
      "tree structure\n",
      "non-rigid surfaces\n",
      "face\n",
      "cloth\n",
      "people\n",
      "spatially complex ASL phenomena\n",
      "classifier predicates\n",
      "interlingua\n",
      "transfer\n",
      "non-terminals\n",
      "schematic variables\n",
      "crossed serial dependencies\n",
      "Dutch subordinate clauses\n",
      "domain knowledge\n",
      "semantic object classes\n",
      "Functional objects\n",
      "discriminative appearance and shape\n",
      "functional objects\n",
      "functional objects\n",
      "people 's trajectories and intents\n",
      "constraint map of the scene\n",
      "locations of functional objects\n",
      "video footage\n",
      "perceptual sounds\n",
      "incoming sound signals\n",
      "acoustic signals of ensemble music\n",
      "information integration\n",
      "translation candidates\n",
      "first-order derivatives of the motion field\n",
      "dynamical changes of the moving objects\n",
      "displacement vector fields ( DVF )\n",
      "CCD sensors\n",
      "spatial nonuni-formity\n",
      "responsivity\n",
      "spatial descriptions\n",
      "qualitative spatial constraints\n",
      "numerical constraints\n",
      "spatial attributes of the entities\n",
      "spatial concepts\n",
      "semantics\n",
      "context information\n",
      "local and holistic contexts\n",
      "local temporal relationship of adjacent frames\n",
      "graph structure of the video\n",
      "priori\n",
      "non-speech interferences ( NSI )\n",
      "zero value of Gaussian process\n",
      "distribution of speech and NSI\n",
      "NSI\n",
      "speaker DOA cues\n",
      "NSI\n",
      "speech sparsity\n",
      "bispectrum amplitude\n",
      "NSI conditions\n",
      "idiosyncrasies\n",
      "patient discharge summary ( PDS )\n",
      "essay-based discourse elements\n",
      "thesis statements\n",
      "Criterion 's capability\n",
      "features\n",
      "discourse structure\n",
      "features\n",
      "breakdowns in coherence\n",
      "discourse elements\n",
      "CURVE-ELEMENT tokens\n",
      "spatially-indexed and scale-indexed data structure\n",
      "image contour\n",
      "local CURVE-ELEMENT attributes\n"
     ]
    }
   ],
   "source": [
    "for row in data:\n",
    "    tokens = row['tokens']\n",
    "    for ent in row['entities']:\n",
    "        s, e, t = ent['start'], ent['end'], ent['type']\n",
    "        if t != 'Other' and not t.istitle():\n",
    "            print(' '.join(tokens[s:e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,22) inside (16,22)\n",
      "tail\n",
      "--------------\n",
      "(16,19) inside (14,19)\n",
      "tail\n",
      "--------------\n",
      "(6,7) inside (6,10)\n",
      "head\n",
      "--------------\n",
      "(17,19) inside (15,19)\n",
      "tail\n",
      "--------------\n",
      "(10,12) inside (8,12)\n",
      "tail\n",
      "--------------\n",
      "(22,25) inside (19,25)\n",
      "tail\n",
      "--------------\n",
      "(6,9) inside (6,13)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (6,10)\n",
      "tail\n",
      "--------------\n",
      "(28,30) inside (25,30)\n",
      "tail\n",
      "--------------\n",
      "(19,21) inside (16,21)\n",
      "tail\n",
      "--------------\n",
      "(14,15) inside (10,15)\n",
      "tail\n",
      "--------------\n",
      "(6,8) inside (3,8)\n",
      "tail\n",
      "--------------\n",
      "(15,16) inside (13,16)\n",
      "tail\n",
      "--------------\n",
      "(6,7) inside (4,7)\n",
      "tail\n",
      "--------------\n",
      "(7,8) inside (4,8)\n",
      "tail\n",
      "--------------\n",
      "(9,13) inside (8,13)\n",
      "tail\n",
      "--------------\n",
      "(34,36) inside (32,36)\n",
      "tail\n",
      "--------------\n",
      "(3,5) inside (0,5)\n",
      "tail\n",
      "--------------\n",
      "(14,16) inside (13,16)\n",
      "tail\n",
      "--------------\n",
      "(31,33) inside (29,33)\n",
      "tail\n",
      "--------------\n",
      "(32,37) inside (28,37)\n",
      "tail\n",
      "--------------\n",
      "(2,3) inside (2,7)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (2,7)\n",
      "else\n",
      "['IA']\n",
      "['QCNs', 'of', 'IA', 'and', 'RCC-8']\n",
      "--------------\n",
      "(6,7) inside (2,7)\n",
      "tail\n",
      "--------------\n",
      "(1,3) inside (0,3)\n",
      "tail\n",
      "--------------\n",
      "(10,12) inside (8,12)\n",
      "tail\n",
      "--------------\n",
      "(20,24) inside (19,24)\n",
      "tail\n",
      "--------------\n",
      "(10,12) inside (9,12)\n",
      "tail\n",
      "--------------\n",
      "(11,13) inside (10,13)\n",
      "tail\n",
      "--------------\n",
      "(15,17) inside (14,17)\n",
      "tail\n",
      "--------------\n",
      "(29,31) inside (28,31)\n",
      "tail\n",
      "--------------\n",
      "(25,26) inside (21,26)\n",
      "tail\n",
      "--------------\n",
      "(15,17) inside (15,20)\n",
      "head\n",
      "--------------\n",
      "(18,20) inside (15,20)\n",
      "tail\n",
      "--------------\n",
      "(6,8) inside (3,8)\n",
      "tail\n",
      "--------------\n",
      "(7,9) inside (5,9)\n",
      "tail\n",
      "--------------\n",
      "(12,13) inside (8,13)\n",
      "tail\n",
      "--------------\n",
      "(15,16) inside (13,16)\n",
      "tail\n",
      "--------------\n",
      "(11,13) inside (10,13)\n",
      "tail\n",
      "--------------\n",
      "(19,21) inside (16,21)\n",
      "tail\n",
      "--------------\n",
      "(9,11) inside (4,14)\n",
      "else\n",
      "['discourse', 'model']\n",
      "['a', 'file', 'card', 'model', 'of', 'discourse', 'model', 'and', 'knowledge', 'store']\n",
      "--------------\n",
      "(12,14) inside (4,14)\n",
      "tail\n",
      "--------------\n",
      "(18,20) inside (14,20)\n",
      "tail\n",
      "--------------\n",
      "(5,8) inside (4,8)\n",
      "tail\n",
      "--------------\n",
      "(19,22) inside (16,22)\n",
      "tail\n",
      "--------------\n",
      "(4,7) inside (1,7)\n",
      "tail\n",
      "--------------\n",
      "(5,7) inside (5,13)\n",
      "head\n",
      "--------------\n",
      "(8,10) inside (5,13)\n",
      "else\n",
      "['machine', 'translation']\n",
      "['automatic', 'evaluation', 'of', 'machine', 'translation', 'and', 'document', 'summarization']\n",
      "--------------\n",
      "(11,13) inside (5,13)\n",
      "tail\n",
      "--------------\n",
      "(18,20) inside (15,20)\n",
      "tail\n",
      "--------------\n",
      "(9,11) inside (9,17)\n",
      "head\n",
      "--------------\n",
      "(12,17) inside (9,17)\n",
      "tail\n",
      "--------------\n",
      "(15,16) inside (12,16)\n",
      "tail\n",
      "--------------\n",
      "(10,12) inside (7,12)\n",
      "tail\n",
      "--------------\n",
      "(12,21) inside (8,21)\n",
      "tail\n",
      "--------------\n",
      "(8,10) inside (8,14)\n",
      "head\n",
      "--------------\n",
      "(11,14) inside (8,14)\n",
      "tail\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(8,10) inside (5,10)\n",
      "tail\n",
      "--------------\n",
      "(40,41) inside (36,41)\n",
      "tail\n",
      "--------------\n",
      "(15,17) inside (13,17)\n",
      "tail\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "match_head = []\n",
    "match_tail = []\n",
    "match_else = []\n",
    "for row in data:\n",
    "    for i, ent in enumerate(row['entities']):\n",
    "        for j, a_ent in enumerate(row['entities']):\n",
    "            if i != j:\n",
    "                if a_ent['start'] >= ent['start'] and a_ent['end'] <= ent['end']:\n",
    "                    flag = False\n",
    "                    print(f\"({a_ent['start']},{a_ent['end']}) inside ({ent['start']},{ent['end']})\")\n",
    "                    \n",
    "                    if a_ent['start'] == ent['start']:\n",
    "                        print('head')\n",
    "                        match_head.append((ent['type'], a_ent['type']))\n",
    "                    elif a_ent['end'] == ent['end']:\n",
    "                        print('tail')\n",
    "                        match_tail.append((ent['type'], a_ent['type']))\n",
    "                    else:\n",
    "                        print('else')\n",
    "                        match_else.append((ent['type'], a_ent['type']))\n",
    "                        print(row['tokens'][a_ent['start']:a_ent['end']])\n",
    "                        print(row['tokens'][ent['start']:ent['end']])\n",
    "                    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Material', 'Material'): 1,\n",
       "         ('OtherScientificTerm', 'OtherScientificTerm'): 2,\n",
       "         ('Method', 'Method'): 1,\n",
       "         ('Task', 'Task'): 3,\n",
       "         ('Method', 'OtherScientificTerm'): 1})"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(match_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Task', 'Method'): 3,\n",
       "         ('Task', 'OtherScientificTerm'): 12,\n",
       "         ('OtherScientificTerm', 'OtherScientificTerm'): 7,\n",
       "         ('Method', 'OtherScientificTerm'): 2,\n",
       "         ('Method', 'Method'): 8,\n",
       "         ('OtherScientificTerm', 'Method'): 2,\n",
       "         ('OtherScientificTerm', 'Generic'): 1,\n",
       "         ('Task', 'Task'): 4,\n",
       "         ('Task', 'Material'): 7,\n",
       "         ('Task', 'Generic'): 2,\n",
       "         ('OtherScientificTerm', 'Material'): 2})"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(match_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Method', 'Method'): 2, ('Task', 'Task'): 1})"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(match_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if every entity is backed by noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check symmetric relations follows word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924 {'tokens': ['Unlike', 'evaluations', 'in', 'the', 'SRE', 'series', ',', 'the', 'i-vector', 'challenge', 'was', 'run', 'entirely', 'online', 'and', 'used', 'fixed-length', 'feature', 'vectors', 'projected', 'into', 'a', 'low-dimensional', 'space', '(', 'i-vectors', ')', 'rather', 'than', 'audio', 'recordings', '.'], 'entities': [{'type': 'Material', 'start': 4, 'end': 6}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'OtherScientificTerm', 'start': 16, 'end': 19}, {'type': 'OtherScientificTerm', 'start': 22, 'end': 27}, {'type': 'Material', 'start': 29, 'end': 31}], 'relations': [{'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 3, 'tail': 1}, {'type': 'Compare', 'head': 4, 'tail': 3}], 'orig_id': '1644e1ee-6a58-4183-962c-2f06fb7dc42e'}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pair = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Conjunction', 'Compare', 'Part-of']:\n",
    "                    \n",
    "            hs, he, ht = entities[rel['head']]['start'], entities[rel['head']]['end'], entities[rel['head']]['type']\n",
    "            ts, te, tt = entities[rel['tail']]['start'], entities[rel['tail']]['end'], entities[rel['tail']]['type']\n",
    "            \n",
    "            if hs > ts and (he-hs) < (te-ts) and ht != 'Generic' and ht != 'OtherScientificTerm' and tt != ht:\n",
    "                count += 1\n",
    "                pair.append((ht, tt))\n",
    "                print(z, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Material', 'OtherScientificTerm'): 1})"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 {'tokens': ['Surprisingly', 'however', ',', 'the', 'WSD', 'accuracy', 'of', 'SMT', 'models', 'has', 'never', 'been', 'evaluated', 'and', 'compared', 'with', 'that', 'of', 'the', 'dedicated', 'WSD', 'models', '.'], 'entities': [{'type': 'Metric', 'start': 4, 'end': 6}, {'type': 'Method', 'start': 7, 'end': 9}, {'type': 'Generic', 'start': 16, 'end': 17}, {'type': 'Method', 'start': 19, 'end': 22}], 'relations': [{'type': 'Evaluate-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 2, 'tail': 0}], 'orig_id': '4bb35994-9317-465a-b5f0-4cb810bb362c'}\n",
      "287 {'tokens': ['We', 'present', 'controlled', 'experiments', 'showing', 'the', 'WSD', 'accuracy', 'of', 'current', 'typical', 'SMT', 'models', 'to', 'be', 'significantly', 'lower', 'than', 'that', 'of', 'all', 'the', 'dedicated', 'WSD', 'models', 'considered', '.'], 'entities': [{'type': 'Metric', 'start': 6, 'end': 8}, {'type': 'Method', 'start': 11, 'end': 13}, {'type': 'Generic', 'start': 18, 'end': 19}, {'type': 'Method', 'start': 22, 'end': 25}], 'relations': [{'type': 'Evaluate-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 2, 'tail': 0}], 'orig_id': '5fbd91c8-b890-4bf6-8375-b8a3860ccbc8'}\n",
      "496 {'tokens': ['Experimental', 'results', 'in', 'the', 'domain', 'of', 'face', 'detection', 'show', 'the', 'training', 'algorithm', 'yields', 'significant', 'improvements', 'in', 'performance', 'over', 'conventional', 'AdaBoost', '.'], 'entities': [{'type': 'Task', 'start': 6, 'end': 8}, {'type': 'Method', 'start': 10, 'end': 12}, {'type': 'Method', 'start': 19, 'end': 20}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 2, 'tail': 0}, {'type': 'Compare', 'head': 2, 'tail': 1}], 'orig_id': '70f30bd5-164f-4945-9176-59535c592041'}\n",
      "592 {'tokens': ['Since', 'the', 'significance', 'of', 'words', 'differs', 'in', 'IR', ',', 'automatic', 'speech', 'recognition', '(', 'ASR', ')', 'performance', 'has', 'been', 'evaluated', 'based', 'on', 'weighted', 'word', 'error', 'rate', '(', 'WWER', ')', ',', 'which', 'gives', 'a', 'weight', 'on', 'errors', 'from', 'the', 'viewpoint', 'of', 'IR', ',', 'instead', 'of', 'word', 'error', 'rate', '(', 'WER', ')', ',', 'which', 'treats', 'all', 'words', 'uniformly', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 2, 'end': 3}, {'type': 'Task', 'start': 7, 'end': 8}, {'type': 'Task', 'start': 9, 'end': 15}, {'type': 'Metric', 'start': 21, 'end': 28}, {'type': 'Task', 'start': 39, 'end': 40}, {'type': 'Metric', 'start': 43, 'end': 49}], 'relations': [{'type': 'Evaluate-for', 'head': 3, 'tail': 2}, {'type': 'Compare', 'head': 5, 'tail': 3}], 'orig_id': 'd6bdadbd-997a-4528-a4ca-e19fdcce0d81'}\n",
      "924 {'tokens': ['Unlike', 'evaluations', 'in', 'the', 'SRE', 'series', ',', 'the', 'i-vector', 'challenge', 'was', 'run', 'entirely', 'online', 'and', 'used', 'fixed-length', 'feature', 'vectors', 'projected', 'into', 'a', 'low-dimensional', 'space', '(', 'i-vectors', ')', 'rather', 'than', 'audio', 'recordings', '.'], 'entities': [{'type': 'Material', 'start': 4, 'end': 6}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'OtherScientificTerm', 'start': 16, 'end': 19}, {'type': 'OtherScientificTerm', 'start': 22, 'end': 27}, {'type': 'Material', 'start': 29, 'end': 31}], 'relations': [{'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 3, 'tail': 1}, {'type': 'Compare', 'head': 4, 'tail': 3}], 'orig_id': '1644e1ee-6a58-4183-962c-2f06fb7dc42e'}\n",
      "1075 {'tokens': ['Our', 'experiments', 'with', 'IARPA-Babel', 'languages', 'show', 'that', 'bottleneck', 'features', 'trained', 'on', 'the', 'most', 'similar', 'source', 'language', 'perform', 'better', 'than', 'those', 'trained', 'on', 'all', 'available', 'source', 'languages', '.'], 'entities': [{'type': 'Material', 'start': 3, 'end': 5}, {'type': 'OtherScientificTerm', 'start': 7, 'end': 9}, {'type': 'Generic', 'start': 19, 'end': 20}], 'relations': [{'type': 'Compare', 'head': 2, 'tail': 1}], 'orig_id': '9f6abc0b-4654-49cc-830a-68481ab612f2'}\n",
      "1239 {'tokens': ['This', 'is', 'the', 'significant', 'advantage', 'of', 'the', 'fluorescence-based', 'method', 'over', 'previous', 'methods', 'based', 'on', 'reflection', '.'], 'entities': [{'type': 'Method', 'start': 7, 'end': 9}, {'type': 'Generic', 'start': 11, 'end': 12}, {'type': 'OtherScientificTerm', 'start': 14, 'end': 15}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 0}], 'orig_id': '47f27805-ba42-474f-b500-461dbdc0b38f'}\n",
      "1482 {'tokens': ['Unlike', 'previous', 'video', 'relighting', 'methods', ',', 'the', 'approach', 'does', 'not', 'assume', 'regions', 'of', 'uniform', 'albedo', ',', 'which', 'makes', 'it', 'applicable', 'to', 'richly', 'textured', 'scenes', '.'], 'entities': [{'type': 'Method', 'start': 2, 'end': 5}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'OtherScientificTerm', 'start': 13, 'end': 15}, {'type': 'Generic', 'start': 18, 'end': 19}, {'type': 'Material', 'start': 21, 'end': 24}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 3, 'tail': 4}], 'orig_id': '3918f006-491a-462d-bd17-47ddd92f3426'}\n"
     ]
    }
   ],
   "source": [
    "# ['Conjunction', 'Compare', 'Part-of']\n",
    "count = 0\n",
    "pair = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Compare']:\n",
    "                    \n",
    "            hs, he, ht = entities[rel['head']]['start'], entities[rel['head']]['end'], entities[rel['head']]['type']\n",
    "            ts, te, tt = entities[rel['tail']]['start'], entities[rel['tail']]['end'], entities[rel['tail']]['type']\n",
    "            \n",
    "            if hs > ts and (he-hs) < (te-ts):\n",
    "                print(z, row)\n",
    "                count += 1\n",
    "#             pair.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['We', 'applied', 'the', 'proposed', 'method', 'to', 'question', 'classification', 'and', 'sentence', 'alignment', 'tasks', 'to', 'evaluate', 'its', 'performance', 'as', 'a', 'similarity', 'measure', 'and', 'a', 'kernel', 'function', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Task', 'start': 6, 'end': 12}, {'type': 'Metric', 'start': 18, 'end': 20}, {'type': 'Method', 'start': 22, 'end': 24}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Conjunction', 'head': 2, 'tail': 3}, {'type': 'Evaluate-for', 'head': 3, 'tail': 0}], 'orig_id': '6d5f52cc-561f-4e5d-aa6c-2f324ae19c32'}\n",
      "{'tokens': ['Our', 'resource-frugal', 'approach', 'results', 'in', '87.5', '%', 'agreement', 'with', 'a', 'state', 'of', 'the', 'art', ',', 'proprietary', 'Arabic', 'stemmer', 'built', 'using', 'rules', ',', 'affix', 'lists', ',', 'and', 'human', 'annotated', 'text', ',', 'in', 'addition', 'to', 'an', 'unsupervised', 'component', '.'], 'entities': [{'type': 'Method', 'start': 1, 'end': 3}, {'type': 'Metric', 'start': 7, 'end': 8}, {'type': 'Method', 'start': 16, 'end': 18}, {'type': 'OtherScientificTerm', 'start': 20, 'end': 21}, {'type': 'Material', 'start': 22, 'end': 24}, {'type': 'Material', 'start': 26, 'end': 29}, {'type': 'Method', 'start': 34, 'end': 36}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Evaluate-for', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 3, 'tail': 2}, {'type': 'Conjunction', 'head': 3, 'tail': 4}, {'type': 'Used-for', 'head': 4, 'tail': 2}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Used-for', 'head': 5, 'tail': 2}, {'type': 'Conjunction', 'head': 5, 'tail': 6}, {'type': 'Used-for', 'head': 6, 'tail': 2}], 'orig_id': 'b0565840-2303-4e8a-bcaf-d5dad4d6af54'}\n",
      "{'tokens': ['This', 'paper', 'discusses', 'three', 'research', 'initiatives', 'at', 'PARC', 'that', 'exemplify', 'these', 'themes', ':', 'a', 'text-image', 'editor', '[', '1', ']', ',', 'a', 'wordspotter', 'for', 'voice', 'editing', 'and', 'indexing', '[', '12', ']', ',', 'and', 'a', 'decoding', 'framework', 'for', 'scanned-document', 'content', 'retrieval', '[', '4', ']', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Generic', 'start': 11, 'end': 12}, {'type': 'Method', 'start': 14, 'end': 16}, {'type': 'Method', 'start': 21, 'end': 22}, {'type': 'Task', 'start': 23, 'end': 27}, {'type': 'Method', 'start': 33, 'end': 35}, {'type': 'Task', 'start': 36, 'end': 39}], 'relations': [{'type': 'Hyponym-of', 'head': 2, 'tail': 0}, {'type': 'Conjunction', 'head': 2, 'tail': 3}, {'type': 'Hyponym-of', 'head': 3, 'tail': 0}, {'type': 'Conjunction', 'head': 3, 'tail': 4}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Hyponym-of', 'head': 5, 'tail': 0}, {'type': 'Used-for', 'head': 5, 'tail': 6}], 'orig_id': '938d7a09-c487-40aa-b3b0-8abb0eed6b20'}\n",
      "{'tokens': ['This', 'paper', 'discusses', 'three', 'research', 'initiatives', 'at', 'PARC', 'that', 'exemplify', 'these', 'themes', ':', 'a', 'text-image', 'editor', '[', '1', ']', ',', 'a', 'wordspotter', 'for', 'voice', 'editing', 'and', 'indexing', '[', '12', ']', ',', 'and', 'a', 'decoding', 'framework', 'for', 'scanned-document', 'content', 'retrieval', '[', '4', ']', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Generic', 'start': 11, 'end': 12}, {'type': 'Method', 'start': 14, 'end': 16}, {'type': 'Method', 'start': 21, 'end': 22}, {'type': 'Task', 'start': 23, 'end': 27}, {'type': 'Method', 'start': 33, 'end': 35}, {'type': 'Task', 'start': 36, 'end': 39}], 'relations': [{'type': 'Hyponym-of', 'head': 2, 'tail': 0}, {'type': 'Conjunction', 'head': 2, 'tail': 3}, {'type': 'Hyponym-of', 'head': 3, 'tail': 0}, {'type': 'Conjunction', 'head': 3, 'tail': 4}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Hyponym-of', 'head': 5, 'tail': 0}, {'type': 'Used-for', 'head': 5, 'tail': 6}], 'orig_id': '938d7a09-c487-40aa-b3b0-8abb0eed6b20'}\n",
      "{'tokens': ['We', 'show', 'that', 'the', 'suggested', 'hybrid', 'proba-bilistic', 'model', '(', 'which', 'combines', 'global', 'variables', ',', 'like', 'translation', ',', 'with', 'local', 'variables', ',', 'like', 'relative', 'positions', 'and', 'appearances', 'of', 'body', 'parts', ')', ',', 'leads', 'to', ':', '(', 'i', ')', 'faster', 'convergence', 'of', 'learning', 'phase', ',', '(', 'ii', ')', 'robustness', 'to', 'occlusions', ',', 'and', ',', '(', 'iii', ')', 'higher', 'recognition', 'rate', '.'], 'entities': [{'type': 'Method', 'start': 5, 'end': 8}, {'type': 'OtherScientificTerm', 'start': 11, 'end': 13}, {'type': 'OtherScientificTerm', 'start': 15, 'end': 16}, {'type': 'OtherScientificTerm', 'start': 18, 'end': 20}, {'type': 'OtherScientificTerm', 'start': 22, 'end': 24}, {'type': 'OtherScientificTerm', 'start': 25, 'end': 29}, {'type': 'Metric', 'start': 37, 'end': 39}, {'type': 'Task', 'start': 40, 'end': 42}, {'type': 'Metric', 'start': 46, 'end': 47}, {'type': 'OtherScientificTerm', 'start': 48, 'end': 49}, {'type': 'Material', 'start': 56, 'end': 58}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Hyponym-of', 'head': 2, 'tail': 1}, {'type': 'Hyponym-of', 'head': 4, 'tail': 3}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Hyponym-of', 'head': 5, 'tail': 3}, {'type': 'Feature-of', 'head': 6, 'tail': 7}, {'type': 'Conjunction', 'head': 6, 'tail': 8}, {'type': 'Conjunction', 'head': 8, 'tail': 10}], 'orig_id': '8ffa7da6-a16e-4532-8603-03e44dc36d97'}\n",
      "{'tokens': ['The', 'demonstrator', 'embodies', 'an', 'interesting', 'combination', 'of', 'hand-built', ',', 'symbolic', 'resources', 'and', 'stochastic', 'processes', '.'], 'entities': [{'type': 'Task', 'start': 1, 'end': 2}, {'type': 'Material', 'start': 7, 'end': 11}, {'type': 'Method', 'start': 12, 'end': 14}], 'relations': [{'type': 'Part-of', 'head': 1, 'tail': 0}, {'type': 'Conjunction', 'head': 1, 'tail': 2}, {'type': 'Part-of', 'head': 2, 'tail': 0}], 'orig_id': '71c16ad7-17f3-4561-a4ca-af5c87a9e692'}\n",
      "{'tokens': ['The', 'recognition', 'system', 'will', 'eventually', 'be', 'integrated', 'with', 'natural', 'language', 'processing', 'to', 'achieve', 'spoken', 'language', 'understanding', '.'], 'entities': [{'type': 'Method', 'start': 1, 'end': 3}, {'type': 'Task', 'start': 8, 'end': 11}, {'type': 'Task', 'start': 13, 'end': 16}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 1, 'tail': 2}], 'orig_id': 'eccd629c-ddc6-4026-b6a1-cecb9952408a'}\n",
      "{'tokens': ['In', 'this', 'paper', 'I', 'will', 'argue', 'for', 'a', 'model', 'of', 'grammatical', 'processing', 'that', 'is', 'based', 'on', 'uniform', 'processing', 'and', 'knowledge', 'sources', '.'], 'entities': [{'type': 'Method', 'start': 8, 'end': 12}, {'type': 'Method', 'start': 16, 'end': 18}, {'type': 'Material', 'start': 19, 'end': 21}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 2, 'tail': 0}, {'type': 'Conjunction', 'head': 2, 'tail': 1}], 'orig_id': 'cc5f514a-9242-4c34-b39c-f9cce6442735'}\n",
      "{'tokens': ['We', 'use', 'a', 'corpus', 'of', 'bracketed', 'sentences', ',', 'called', 'a', 'Treebank', ',', 'in', 'combination', 'with', 'decision', 'tree', 'building', 'to', 'tease', 'out', 'the', 'relevant', 'aspects', 'of', 'a', 'parse', 'tree', 'that', 'will', 'determine', 'the', 'correct', 'parse', 'of', 'a', 'sentence', '.'], 'entities': [{'type': 'Material', 'start': 3, 'end': 7}, {'type': 'Material', 'start': 10, 'end': 11}, {'type': 'Method', 'start': 15, 'end': 18}, {'type': 'OtherScientificTerm', 'start': 26, 'end': 28}, {'type': 'OtherScientificTerm', 'start': 33, 'end': 34}], 'relations': [{'type': 'Conjunction', 'head': 0, 'tail': 2}, {'type': 'Used-for', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 3, 'tail': 4}], 'orig_id': 'b763acb4-cbdc-46f2-91ae-e24c3357363a'}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pair_dist = []\n",
    "pair_type = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Conjunction']:\n",
    "            flag = False\n",
    "                    \n",
    "            hs, he, ht = entities[rel['head']]['start'], entities[rel['head']]['end'], entities[rel['head']]['type']\n",
    "            ts, te, tt = entities[rel['tail']]['start'], entities[rel['tail']]['end'], entities[rel['tail']]['type']\n",
    "            \n",
    "            pair_type.append((ht,tt))\n",
    "            \n",
    "            if ht != tt and (ht != 'Generic' and tt != 'Generic' and \n",
    "                             ht != 'OtherScientificTerm' and tt != 'OtherScientificTerm'):\n",
    "                print(row)\n",
    "            \n",
    "            if hs < ts:\n",
    "                pair_dist.append(ts-he)\n",
    "                \n",
    "#                 if ts - he == 1:\n",
    "#                     print(row['tokens'][he])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 233,\n",
       "         2: 69,\n",
       "         8: 5,\n",
       "         5: 16,\n",
       "         3: 17,\n",
       "         4: 9,\n",
       "         6: 2,\n",
       "         7: 6,\n",
       "         10: 2,\n",
       "         11: 3,\n",
       "         9: 3,\n",
       "         13: 1,\n",
       "         16: 1,\n",
       "         22: 1})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pair_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('OtherScientificTerm', 'OtherScientificTerm'): 113,\n",
       "         ('Task', 'Task'): 83,\n",
       "         ('Generic', 'Generic'): 6,\n",
       "         ('Metric', 'Metric'): 24,\n",
       "         ('Method', 'Method'): 65,\n",
       "         ('Material', 'Material'): 49,\n",
       "         ('Generic', 'Method'): 7,\n",
       "         ('Generic', 'OtherScientificTerm'): 4,\n",
       "         ('OtherScientificTerm', 'Method'): 12,\n",
       "         ('Method', 'OtherScientificTerm'): 11,\n",
       "         ('Metric', 'Generic'): 1,\n",
       "         ('Metric', 'Method'): 1,\n",
       "         ('Method', 'Generic'): 5,\n",
       "         ('Metric', 'OtherScientificTerm'): 2,\n",
       "         ('Material', 'Generic'): 1,\n",
       "         ('OtherScientificTerm', 'Material'): 2,\n",
       "         ('Material', 'Method'): 4,\n",
       "         ('Method', 'Task'): 1,\n",
       "         ('Task', 'Method'): 2,\n",
       "         ('Generic', 'Material'): 1,\n",
       "         ('Metric', 'Material'): 1,\n",
       "         ('Material', 'OtherScientificTerm'): 2,\n",
       "         ('Task', 'OtherScientificTerm'): 1,\n",
       "         ('OtherScientificTerm', 'Generic'): 1,\n",
       "         ('OtherScientificTerm', 'Metric'): 1})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pair_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pari_type = defaultdict(Counter)\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        pari_type[rel['type']][(entities[rel['head']]['type'], \n",
    "                                    entities[rel['tail']]['type'])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'Conjunction': Counter({('OtherScientificTerm',\n",
       "                       'OtherScientificTerm'): 113,\n",
       "                      ('Task', 'Task'): 83,\n",
       "                      ('Generic', 'Generic'): 6,\n",
       "                      ('Metric', 'Metric'): 24,\n",
       "                      ('Method', 'Method'): 65,\n",
       "                      ('Material', 'Material'): 49,\n",
       "                      ('Generic', 'Method'): 7,\n",
       "                      ('Generic', 'OtherScientificTerm'): 4,\n",
       "                      ('OtherScientificTerm', 'Method'): 12,\n",
       "                      ('Method', 'OtherScientificTerm'): 11,\n",
       "                      ('Metric', 'Generic'): 1,\n",
       "                      ('Metric', 'Method'): 1,\n",
       "                      ('Method', 'Generic'): 5,\n",
       "                      ('Metric', 'OtherScientificTerm'): 2,\n",
       "                      ('Material', 'Generic'): 1,\n",
       "                      ('OtherScientificTerm', 'Material'): 2,\n",
       "                      ('Material', 'Method'): 4,\n",
       "                      ('Method', 'Task'): 1,\n",
       "                      ('Task', 'Method'): 2,\n",
       "                      ('Generic', 'Material'): 1,\n",
       "                      ('Metric', 'Material'): 1,\n",
       "                      ('Material', 'OtherScientificTerm'): 2,\n",
       "                      ('Task', 'OtherScientificTerm'): 1,\n",
       "                      ('OtherScientificTerm', 'Generic'): 1,\n",
       "                      ('OtherScientificTerm', 'Metric'): 1}),\n",
       "             'Feature-of': Counter({('OtherScientificTerm', 'Material'): 11,\n",
       "                      ('OtherScientificTerm', 'OtherScientificTerm'): 53,\n",
       "                      ('Material', 'Generic'): 5,\n",
       "                      ('OtherScientificTerm', 'Generic'): 16,\n",
       "                      ('Metric', 'Generic'): 1,\n",
       "                      ('Material', 'OtherScientificTerm'): 7,\n",
       "                      ('Method', 'Task'): 1,\n",
       "                      ('Material', 'Material'): 9,\n",
       "                      ('OtherScientificTerm', 'Method'): 20,\n",
       "                      ('Method', 'OtherScientificTerm'): 4,\n",
       "                      ('Metric', 'Method'): 1,\n",
       "                      ('Task', 'Generic'): 2,\n",
       "                      ('OtherScientificTerm', 'Task'): 18,\n",
       "                      ('Method', 'Method'): 1,\n",
       "                      ('Material', 'Method'): 1,\n",
       "                      ('Task', 'Task'): 2,\n",
       "                      ('Metric', 'Metric'): 1,\n",
       "                      ('OtherScientificTerm', 'Metric'): 3,\n",
       "                      ('Generic', 'OtherScientificTerm'): 2,\n",
       "                      ('Task', 'Method'): 3,\n",
       "                      ('Metric', 'Material'): 1,\n",
       "                      ('Method', 'Generic'): 2,\n",
       "                      ('Generic', 'Task'): 1,\n",
       "                      ('Metric', 'Task'): 4,\n",
       "                      ('Metric', 'OtherScientificTerm'): 1,\n",
       "                      ('Material', 'Task'): 3}),\n",
       "             'Hyponym-of': Counter({('Material', 'Material'): 22,\n",
       "                      ('OtherScientificTerm', 'OtherScientificTerm'): 66,\n",
       "                      ('Generic', 'Method'): 8,\n",
       "                      ('Metric', 'Metric'): 7,\n",
       "                      ('Method', 'Method'): 55,\n",
       "                      ('Material', 'Generic'): 20,\n",
       "                      ('Task', 'Generic'): 28,\n",
       "                      ('OtherScientificTerm', 'Generic'): 16,\n",
       "                      ('Method', 'Generic'): 17,\n",
       "                      ('Task', 'Task'): 31,\n",
       "                      ('OtherScientificTerm', 'Method'): 4,\n",
       "                      ('Method', 'Task'): 1,\n",
       "                      ('Generic', 'Generic'): 7,\n",
       "                      ('Material', 'Task'): 3,\n",
       "                      ('OtherScientificTerm', 'Task'): 1,\n",
       "                      ('Generic', 'Task'): 1,\n",
       "                      ('Method', 'OtherScientificTerm'): 2,\n",
       "                      ('OtherScientificTerm', 'Material'): 3,\n",
       "                      ('Material', 'OtherScientificTerm'): 3,\n",
       "                      ('Generic', 'OtherScientificTerm'): 2,\n",
       "                      ('Metric', 'OtherScientificTerm'): 1}),\n",
       "             'Used-for': Counter({('Method', 'OtherScientificTerm'): 153,\n",
       "                      ('OtherScientificTerm', 'Task'): 52,\n",
       "                      ('Method', 'Task'): 246,\n",
       "                      ('Method', 'Method'): 140,\n",
       "                      ('Generic', 'Task'): 182,\n",
       "                      ('Method', 'Material'): 38,\n",
       "                      ('Generic', 'Generic'): 33,\n",
       "                      ('OtherScientificTerm', 'Method'): 93,\n",
       "                      ('Generic', 'OtherScientificTerm'): 94,\n",
       "                      ('OtherScientificTerm', 'Metric'): 6,\n",
       "                      ('Method', 'Generic'): 97,\n",
       "                      ('Material', 'Generic'): 44,\n",
       "                      ('Material', 'OtherScientificTerm'): 20,\n",
       "                      ('OtherScientificTerm', 'OtherScientificTerm'): 93,\n",
       "                      ('Material', 'Method'): 71,\n",
       "                      ('OtherScientificTerm', 'Generic'): 71,\n",
       "                      ('Task', 'Generic'): 23,\n",
       "                      ('Generic', 'Method'): 45,\n",
       "                      ('Task', 'OtherScientificTerm'): 16,\n",
       "                      ('Material', 'Task'): 47,\n",
       "                      ('Task', 'Task'): 42,\n",
       "                      ('Task', 'Material'): 5,\n",
       "                      ('OtherScientificTerm', 'Material'): 9,\n",
       "                      ('Generic', 'Material'): 16,\n",
       "                      ('Metric', 'Generic'): 2,\n",
       "                      ('Task', 'Metric'): 1,\n",
       "                      ('Metric', 'Task'): 4,\n",
       "                      ('Metric', 'OtherScientificTerm'): 11,\n",
       "                      ('Metric', 'Method'): 5,\n",
       "                      ('Material', 'Material'): 14,\n",
       "                      ('Task', 'Method'): 10,\n",
       "                      ('Method', 'Metric'): 4,\n",
       "                      ('Metric', 'Material'): 1,\n",
       "                      ('Generic', 'Metric'): 1,\n",
       "                      ('Material', 'Metric'): 1}),\n",
       "             'Part-of': Counter({('OtherScientificTerm', 'Method'): 17,\n",
       "                      ('Task', 'Task'): 13,\n",
       "                      ('Method', 'Method'): 31,\n",
       "                      ('Generic', 'OtherScientificTerm'): 4,\n",
       "                      ('Generic', 'Method'): 8,\n",
       "                      ('OtherScientificTerm', 'Material'): 12,\n",
       "                      ('Material', 'Material'): 4,\n",
       "                      ('Method', 'Generic'): 21,\n",
       "                      ('Task', 'Generic'): 6,\n",
       "                      ('OtherScientificTerm', 'Task'): 7,\n",
       "                      ('Generic', 'Material'): 1,\n",
       "                      ('OtherScientificTerm', 'OtherScientificTerm'): 18,\n",
       "                      ('Generic', 'Generic'): 9,\n",
       "                      ('OtherScientificTerm', 'Generic'): 13,\n",
       "                      ('Generic', 'Task'): 3,\n",
       "                      ('Method', 'OtherScientificTerm'): 5,\n",
       "                      ('OtherScientificTerm', 'Metric'): 1,\n",
       "                      ('Metric', 'Generic'): 1,\n",
       "                      ('Material', 'Generic'): 2,\n",
       "                      ('Material', 'Task'): 1,\n",
       "                      ('Method', 'Task'): 2}),\n",
       "             'Compare': Counter({('Generic', 'Generic'): 33,\n",
       "                      ('OtherScientificTerm', 'OtherScientificTerm'): 17,\n",
       "                      ('Method', 'Method'): 50,\n",
       "                      ('Generic', 'Method'): 29,\n",
       "                      ('Method', 'Generic'): 9,\n",
       "                      ('Generic', 'Metric'): 3,\n",
       "                      ('Generic', 'OtherScientificTerm'): 7,\n",
       "                      ('Metric', 'Metric'): 2,\n",
       "                      ('Task', 'Task'): 4,\n",
       "                      ('Material', 'OtherScientificTerm'): 1,\n",
       "                      ('Material', 'Material'): 4,\n",
       "                      ('Material', 'Task'): 1,\n",
       "                      ('Generic', 'Material'): 2,\n",
       "                      ('Metric', 'Generic'): 1,\n",
       "                      ('OtherScientificTerm', 'Material'): 1,\n",
       "                      ('OtherScientificTerm', 'Generic'): 1,\n",
       "                      ('Generic', 'Task'): 1}),\n",
       "             'Evaluate-for': Counter({('Metric', 'Material'): 2,\n",
       "                      ('Metric', 'Generic'): 66,\n",
       "                      ('Metric', 'Method'): 59,\n",
       "                      ('Method', 'OtherScientificTerm'): 2,\n",
       "                      ('Generic', 'Generic'): 10,\n",
       "                      ('Metric', 'OtherScientificTerm'): 14,\n",
       "                      ('Metric', 'Task'): 21,\n",
       "                      ('Task', 'Generic'): 30,\n",
       "                      ('Task', 'Task'): 3,\n",
       "                      ('Material', 'Method'): 19,\n",
       "                      ('Task', 'Method'): 10,\n",
       "                      ('Material', 'Generic'): 28,\n",
       "                      ('Method', 'Method'): 4,\n",
       "                      ('Generic', 'Method'): 15,\n",
       "                      ('Material', 'Task'): 8,\n",
       "                      ('Task', 'OtherScientificTerm'): 2,\n",
       "                      ('Method', 'Generic'): 4,\n",
       "                      ('Generic', 'OtherScientificTerm'): 1,\n",
       "                      ('Generic', 'Task'): 5,\n",
       "                      ('Task', 'Metric'): 3,\n",
       "                      ('Material', 'Metric'): 4,\n",
       "                      ('OtherScientificTerm', 'Method'): 2,\n",
       "                      ('Method', 'Metric'): 1})})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pari_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Thirdly the learned intrinsic object structure is integrated into a particle-filter style tracker .\n",
      "intrinsic object structure\n",
      "particle-filter style tracker\n",
      "\n",
      "\n",
      "23\n",
      "Graph unification remains the most expensive part of unification-based grammar parsing .\n",
      "Graph unification\n",
      "unification-based grammar parsing\n",
      "\n",
      "\n",
      "24\n",
      "We focus on one speed-up element in the design of unification algorithms : avoidance of copying of unmodified subgraphs .\n",
      "speed-up element\n",
      "unification algorithms\n",
      "\n",
      "\n",
      "60\n",
      "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different viewpoints , various resolutions and changing illuminations .\n",
      "dominant factors\n",
      "domain divergence\n",
      "\n",
      "\n",
      "64\n",
      "To better couple the two denoising auto-encoders learning , we incorporate a feature mapping , which tends to transfer knowledge between the intermediate domain and the target one .\n",
      "feature mapping\n",
      "denoising auto-encoders learning\n",
      "\n",
      "\n",
      "68\n",
      "Basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a linear combination of these patterns expresses a particular personalized aging process .\n",
      "dictionary bases\n",
      "age-group specific dictionaries\n",
      "\n",
      "\n",
      "91\n",
      "Constraint propagation is one of the key techniques in constraint programming , and a large body of work has built up around it .\n",
      "Constraint propagation\n",
      "constraint programming\n",
      "\n",
      "\n",
      "107\n",
      "This paper solves the automatic initial-ization problem by performing boosted shape detection as a generic measurement process and integrating it in our tracking framework .\n",
      "it\n",
      "tracking framework\n",
      "\n",
      "\n",
      "110\n",
      "Our framework is applied for the automatic tracking of endocardium in ultrasound sequences of the human heart .\n",
      "endocardium\n",
      "ultrasound sequences of the human heart\n",
      "\n",
      "\n",
      "129\n",
      "This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams ( 87.2 % -96.7 % accuracy depending on classification method ) .\n",
      "medium-length speeches\n",
      "EUROPARL corpus\n",
      "\n",
      "\n",
      "137\n",
      "This task involves two core technologies : natural language processing ( NLP ) and information extraction ( IE ) .\n",
      "natural language processing ( NLP )\n",
      "task\n",
      "\n",
      "\n",
      "137\n",
      "This task involves two core technologies : natural language processing ( NLP ) and information extraction ( IE ) .\n",
      "information extraction ( IE )\n",
      "task\n",
      "\n",
      "\n",
      "147\n",
      "These words appear frequently enough in dialog to warrant serious attention , yet present natural language search engines perform poorly on queries containing them .\n",
      "words\n",
      "dialog\n",
      "\n",
      "\n",
      "148\n",
      "I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine 's operational semantics .\n",
      "approximation of the formal analysis\n",
      "search engine\n",
      "\n",
      "\n",
      "148\n",
      "I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine 's operational semantics .\n",
      "operational semantics\n",
      "search engine\n",
      "\n",
      "\n",
      "149\n",
      "The value of this approach is that as the operational semantics of natural language applications improve , even larger improvements are possible .\n",
      "operational semantics\n",
      "natural language applications\n",
      "\n",
      "\n",
      "172\n",
      "This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1 .\n",
      "tasks\n",
      "SUMMAC-1\n",
      "\n",
      "\n",
      "199\n",
      "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important extensions over standard belief propagation ( BP ) : '' priority-based message scheduling '' and '' dynamic label pruning '' .\n",
      "extensions\n",
      "optimization scheme\n",
      "\n",
      "\n",
      "211\n",
      "We present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of images with unknown motions .\n",
      "unknown motions\n",
      "images\n",
      "\n",
      "\n",
      "233\n",
      "In this paper , we present Photogeometric Structured Light whereby a standard structured light method is extended to include photometric methods .\n",
      "structured light method\n",
      "Photogeometric Structured Light\n",
      "\n",
      "\n",
      "233\n",
      "In this paper , we present Photogeometric Structured Light whereby a standard structured light method is extended to include photometric methods .\n",
      "photometric methods\n",
      "Photogeometric Structured Light\n",
      "\n",
      "\n",
      "303\n",
      "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : ( 1 ) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , ( 2 ) for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and ( 3 ) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .\n",
      "predicting subtopic boundaries\n",
      "predicting top-level and predicting subtopic boundaries\n",
      "\n",
      "\n",
      "303\n",
      "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : ( 1 ) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , ( 2 ) for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and ( 3 ) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .\n",
      "predicting top-level boundaries\n",
      "predicting top-level and predicting subtopic boundaries\n",
      "\n",
      "\n",
      "305\n",
      "We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton .\n",
      "hubs\n",
      "automaton\n",
      "\n",
      "\n",
      "306\n",
      "For our purposes , a hub is a node in a graph with in-degree greater than one and out-degree greater than one .\n",
      "node\n",
      "graph\n",
      "\n",
      "\n",
      "309\n",
      "In Bayesian machine learning , conjugate priors are popular , mostly due to mathematical convenience .\n",
      "conjugate priors\n",
      "Bayesian machine learning\n",
      "\n",
      "\n",
      "313\n",
      "We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning .\n",
      "generative and discriminative components\n",
      "hybrid model\n",
      "\n",
      "\n",
      "315\n",
      "This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables .\n",
      "non-terminal symbols\n",
      "PCFG\n",
      "\n",
      "\n",
      "329\n",
      "Dictionary construction , one of the most difficult tasks in developing a machine translation system , is expensive .\n",
      "Dictionary construction\n",
      "machine translation system\n",
      "\n",
      "\n",
      "365\n",
      "We show that AS is a particular instance of the Ant-Q family , and that there are instances of this family which perform better than AS .\n",
      "instances\n",
      "family\n",
      "\n",
      "\n",
      "369\n",
      "In our framework , the structures of classes are conceptualized as a semi-Riemannian manifold which is considered as a submanifold embedded in an ambient semi-Riemannian space .\n",
      "submanifold\n",
      "ambient semi-Riemannian space\n",
      "\n",
      "\n",
      "377\n",
      "A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components .\n",
      "symbolic and connectionist components\n",
      "it\n",
      "\n",
      "\n",
      "380\n",
      "Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process .\n",
      "connectionist component\n",
      "parsing process\n",
      "\n",
      "\n",
      "386\n",
      "A parsing algorithm is presented that integrates several different parsing strategies , with case-frame instantiation dominating .\n",
      "parsing strategies\n",
      "parsing algorithm\n",
      "\n",
      "\n",
      "388\n",
      "Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework .\n",
      "specific heuristics\n",
      "multi-strategy framework\n",
      "\n",
      "\n",
      "423\n",
      "This topology is then incorporated into the Hidden Conditional Ordinal Random Field ( H-CORF ) framework for dynamic ordinal regression by constraining H-CORF parameters to lie on the ordinal manifold .\n",
      "topology\n",
      "Hidden Conditional Ordinal Random Field ( H-CORF ) framework\n",
      "\n",
      "\n",
      "449\n",
      "The backbone of the annotation are semantic roles in the frame semantics paradigm .\n",
      "semantic roles\n",
      "frame semantics paradigm\n",
      "\n",
      "\n",
      "478\n",
      "How to obtain hierarchical relations ( e.g. superordinate - hyponym relation , synonym relation ) is one of the most important problems for thesaurus construction .\n",
      "hierarchical relations\n",
      "thesaurus construction\n",
      "\n",
      "\n",
      "480\n",
      "The features of the definition sentences in the dictionary , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .\n",
      "definition sentences\n",
      "dictionary\n",
      "\n",
      "\n",
      "498\n",
      "This paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities .\n",
      "multiple layers of linear projections\n",
      "two-branch neural network\n",
      "\n",
      "\n",
      "498\n",
      "This paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities .\n",
      "nonlinearities\n",
      "two-branch neural network\n",
      "\n",
      "\n",
      "514\n",
      "In the security domain a key problem is identifying rare behaviours of interest .\n",
      "identifying rare behaviours of interest\n",
      "security domain\n",
      "\n",
      "\n",
      "520\n",
      "The model is embodied in a program , APT , that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .\n",
      "model\n",
      "program\n",
      "\n",
      "\n",
      "539\n",
      "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .\n",
      "arguments\n",
      "formalization of defeasible reasoning\n",
      "\n",
      "\n",
      "539\n",
      "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .\n",
      "defeat rules\n",
      "formalization of defeasible reasoning\n",
      "\n",
      "\n",
      "557\n",
      "Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications .\n",
      "themes\n",
      "speech and text image processing\n",
      "\n",
      "\n",
      "570\n",
      "We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system .\n",
      "embodied conversational agent\n",
      "dialogue system\n",
      "\n",
      "\n",
      "606\n",
      "We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure .\n",
      "phrase translations\n",
      "suffix array-based data structure\n",
      "\n",
      "\n",
      "643\n",
      "The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code .\n",
      "Intel i860 chip\n",
      "board\n",
      "\n",
      "\n",
      "644\n",
      "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .\n",
      "VME bus\n",
      "SUN4\n",
      "\n",
      "\n",
      "644\n",
      "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .\n",
      "natural language system\n",
      "board\n",
      "\n",
      "\n",
      "644\n",
      "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .\n",
      "application back end\n",
      "board\n",
      "\n",
      "\n",
      "665\n",
      "Our approach is based on the use of a relational probability model to define a generative model for the domain , including models of author and title corruption and a probabilistic citation grammar .\n",
      "models of author and title corruption\n",
      "relational probability model\n",
      "\n",
      "\n",
      "665\n",
      "Our approach is based on the use of a relational probability model to define a generative model for the domain , including models of author and title corruption and a probabilistic citation grammar .\n",
      "probabilistic citation grammar\n",
      "relational probability model\n",
      "\n",
      "\n",
      "675\n",
      "In this study , we propose a knowledge-independent method for aligning terms and thus extracting translations from a small , domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong .\n",
      "parallel English and Chinese court judgments\n",
      "small , domain-specific corpus\n",
      "\n",
      "\n",
      "676\n",
      "With a sentence-aligned corpus , translation equivalences are suggested by analysing the frequency profiles of parallel concordances .\n",
      "frequency profiles\n",
      "parallel concordances\n",
      "\n",
      "\n",
      "684\n",
      "Leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into geometric classes , based on predictions made by region-classifiers that are trained on appearance and motion features .\n",
      "geometric classes\n",
      "dynamic scene\n",
      "\n",
      "\n",
      "686\n",
      "We built a novel , extensive dataset on geometric context of video to evaluate our method , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .\n",
      "annotated outdoor videos\n",
      "dataset\n",
      "\n",
      "\n",
      "697\n",
      "We go , on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system .\n",
      "flexibilities\n",
      "limited-domain computer system\n",
      "\n",
      "\n",
      "713\n",
      "An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English .\n",
      "mechanism\n",
      "system\n",
      "\n",
      "\n",
      "728\n",
      "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations ( synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment ) as labeled pointers between word senses .\n",
      "semantic relations\n",
      "WordNet\n",
      "\n",
      "\n",
      "736\n",
      "This paper describes the detailed construction of the transfer phase of our system from Japanese to English , and gives some examples of problems which seem difficult to treat in the interlingual approach .\n",
      "transfer phase\n",
      "system\n",
      "\n",
      "\n",
      "737\n",
      "The basic design principles of the transfer phase of our system have already been mentioned in ( 1 ) ( 2 ) .\n",
      "transfer phase\n",
      "system\n",
      "\n",
      "\n",
      "738\n",
      "Some of the principles which are relevant to the topic of this paper are : ( a ) Multiple Layer of Grammars ( b ) Multiple Layer Presentation ( c ) Lexicon Driven Processing ( d ) Form-Oriented Dictionary Description .\n",
      "Multiple Layer of Grammars\n",
      "principles\n",
      "\n",
      "\n",
      "738\n",
      "Some of the principles which are relevant to the topic of this paper are : ( a ) Multiple Layer of Grammars ( b ) Multiple Layer Presentation ( c ) Lexicon Driven Processing ( d ) Form-Oriented Dictionary Description .\n",
      "Multiple Layer Presentation\n",
      "principles\n",
      "\n",
      "\n",
      "738\n",
      "Some of the principles which are relevant to the topic of this paper are : ( a ) Multiple Layer of Grammars ( b ) Multiple Layer Presentation ( c ) Lexicon Driven Processing ( d ) Form-Oriented Dictionary Description .\n",
      "Lexicon Driven Processing\n",
      "principles\n",
      "\n",
      "\n",
      "738\n",
      "Some of the principles which are relevant to the topic of this paper are : ( a ) Multiple Layer of Grammars ( b ) Multiple Layer Presentation ( c ) Lexicon Driven Processing ( d ) Form-Oriented Dictionary Description .\n",
      "Form-Oriented Dictionary Description\n",
      "principles\n",
      "\n",
      "\n",
      "739\n",
      "This paper also shows how these principles are realized in the current system .\n",
      "principles\n",
      "system\n",
      "\n",
      "\n",
      "742\n",
      "This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish .\n",
      "linguistic PoS tagger and chunker\n",
      "broad coverage unification based grammar of Spanish\n",
      "\n",
      "\n",
      "755\n",
      "In this paper , we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .\n",
      "nouns\n",
      "SENSEVAL-2 English lexical sample task\n",
      "\n",
      "\n",
      "759\n",
      "This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .\n",
      "quantification over events\n",
      "temporal anaphora\n",
      "\n",
      "\n",
      "760\n",
      "The analysis in ( Partee , 1984 ) of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .\n",
      "temporal connective\n",
      "subordinate clause\n",
      "\n",
      "\n",
      "766\n",
      "Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .\n",
      "mapping score\n",
      "correlation measure\n",
      "\n",
      "\n",
      "767\n",
      "The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training .\n",
      "correlations\n",
      "Maximum Entropy-based ranking model\n",
      "\n",
      "\n",
      "794\n",
      "We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints .\n",
      "asymmetric map-uniqueness criterion\n",
      "formulation\n",
      "\n",
      "\n",
      "808\n",
      "Then , an improved SVM like learning system incorporating the hypergraph regularization , called Rank-HLapSVM , is proposed to handle the multi-label classification problems .\n",
      "hypergraph regularization\n",
      "SVM like learning system\n",
      "\n",
      "\n",
      "839\n",
      "Sentence planning is a set of inter-related but distinct tasks , one of which is sentence scoping , i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences .\n",
      "sentence scoping\n",
      "tasks\n",
      "\n",
      "\n",
      "854\n",
      "The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .\n",
      "domain independent acoustic models\n",
      "method\n",
      "\n",
      "\n",
      "854\n",
      "The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .\n",
      "classifiers\n",
      "method\n",
      "\n",
      "\n",
      "855\n",
      "In our method , unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .\n",
      "unsupervised training\n",
      "method\n",
      "\n",
      "\n",
      "860\n",
      "Our algorithm considers chordal QCNs and a new form of partial consistency which we define as ◆ G-consistency .\n",
      "chordal QCNs\n",
      "algorithm\n",
      "\n",
      "\n",
      "860\n",
      "Our algorithm considers chordal QCNs and a new form of partial consistency which we define as ◆ G-consistency .\n",
      "partial consistency\n",
      "algorithm\n",
      "\n",
      "\n",
      "880\n",
      "The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing .\n",
      "rule-invocation strategies\n",
      "context-free chart parsing\n",
      "\n",
      "\n",
      "891\n",
      "Understanding natural images has been extensively studied in computer vision , while diagram understanding has received little attention .\n",
      "Understanding natural images\n",
      "computer vision\n",
      "\n",
      "\n",
      "930\n",
      "In this paper , we will introduce the anaphoric component of the Mimo formalism .\n",
      "anaphoric component\n",
      "Mimo formalism\n",
      "\n",
      "\n",
      "949\n",
      "The estimation is then used to select the best acoustic model out of a library of models trained in various artificial re-verberant conditions .\n",
      "acoustic model\n",
      "models\n",
      "\n",
      "\n",
      "955\n",
      "From this , a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context .\n",
      "language learning model\n",
      "RINA\n",
      "\n",
      "\n",
      "956\n",
      "We identified two tasks : First , how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers [ Zernik87 ] .\n",
      "linguistic concepts\n",
      "hierarchy\n",
      "\n",
      "\n",
      "962\n",
      "Furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .\n",
      "pronouns\n",
      "unannotated text\n",
      "\n",
      "\n",
      "965\n",
      "Although Bikel 's parser achieves a higher accuracy for parsing written language , it achieves a higher accuracy when extracting subcategorization cues from spoken language .\n",
      "subcategorization cues\n",
      "spoken language\n",
      "\n",
      "\n",
      "968\n",
      "Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language .\n",
      "subcategorization cues\n",
      "spoken language\n",
      "\n",
      "\n",
      "991\n",
      "We present a single-image highlight removal method that incorporates illumination-based constraints into image in-painting .\n",
      "illumination-based constraints\n",
      "image in-painting\n",
      "\n",
      "\n",
      "997\n",
      "An objective function is defined to impose lo-calization constraint , in addition to the non-negativity constraint in the standard NMF [ 1 ] .\n",
      "non-negativity constraint\n",
      "NMF\n",
      "\n",
      "\n",
      "1008\n",
      "In this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque .\n",
      "comma checker\n",
      "grammar checker\n",
      "\n",
      "\n",
      "1014\n",
      "Our aim is to revisit the present-day syntactico-semantic ( tectogrammatical ) annotation in the Prague Dependency Treebank , extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new , discourse level of annotation .\n",
      "syntactico-semantic ( tectogrammatical ) annotation\n",
      "Prague Dependency Treebank\n",
      "\n",
      "\n",
      "1024\n",
      "This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .\n",
      "layers\n",
      "images\n",
      "\n",
      "\n",
      "1024\n",
      "This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .\n",
      "planar patches\n",
      "scene\n",
      "\n",
      "\n",
      "1025\n",
      "Layers in the input images will be mapped in the subspace , where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm .\n",
      "Layers\n",
      "images\n",
      "\n",
      "\n",
      "1033\n",
      "We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .\n",
      "extraneous edges\n",
      "causal graph\n",
      "\n",
      "\n",
      "1038\n",
      "We show that there is unambiguous association between visual content and natural language descriptions in our dataset , making it an ideal benchmark for the visual content captioning task .\n",
      "visual content\n",
      "dataset\n",
      "\n",
      "\n",
      "1038\n",
      "We show that there is unambiguous association between visual content and natural language descriptions in our dataset , making it an ideal benchmark for the visual content captioning task .\n",
      "natural language descriptions\n",
      "dataset\n",
      "\n",
      "\n",
      "1084\n",
      "In the phase of training , a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images .\n",
      "blob-tokens\n",
      "visual vocabulary\n",
      "\n",
      "\n",
      "1115\n",
      "Multimedia answers include videodisc images and heuristically-produced complete sentences in text or text-to-speech form .\n",
      "videodisc images\n",
      "Multimedia answers\n",
      "\n",
      "\n",
      "1118\n",
      "The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality .\n",
      "general-purpose NLP components\n",
      "machine translation pipeline\n",
      "\n",
      "\n",
      "1119\n",
      "The demonstrator embodies an interesting combination of hand-built , symbolic resources and stochastic processes .\n",
      "hand-built , symbolic resources\n",
      "demonstrator\n",
      "\n",
      "\n",
      "1119\n",
      "The demonstrator embodies an interesting combination of hand-built , symbolic resources and stochastic processes .\n",
      "stochastic processes\n",
      "demonstrator\n",
      "\n",
      "\n",
      "1148\n",
      "In this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting images containing blurred regions and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .\n",
      "blurred regions\n",
      "images\n",
      "\n",
      "\n",
      "1166\n",
      "Background maintenance is a frequent element of video surveillance systems .\n",
      "Background maintenance\n",
      "video surveillance systems\n",
      "\n",
      "\n",
      "1167\n",
      "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .\n",
      "pixel-level component\n",
      "three-component system\n",
      "\n",
      "\n",
      "1167\n",
      "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .\n",
      "region-level component\n",
      "three-component system\n",
      "\n",
      "\n",
      "1167\n",
      "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .\n",
      "frame-level component\n",
      "three-component system\n",
      "\n",
      "\n",
      "1175\n",
      "First , starting from the WSJ-trained recognizer , how much adaptation data ( taken from the Phonebook training corpus ) is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?\n",
      "adaptation data\n",
      "Phonebook training corpus\n",
      "\n",
      "\n",
      "1182\n",
      "It was implemented in the IE module of FACILE , a EU project for multilingual text classification and IE .\n",
      "IE module\n",
      "FACILE , a EU project for multilingual text classification and IE\n",
      "\n",
      "\n",
      "1190\n",
      "Unlike the quantitative prior , the qualitative prior is often ignored due to the difficulty of incorporating them into the model learning process .\n",
      "them\n",
      "model learning process\n",
      "\n",
      "\n",
      "1199\n",
      "We describe a new approach which involves clustering subcategorization frame ( SCF ) distributions using the Information Bottleneck and nearest neighbour methods .\n",
      "clustering subcategorization frame ( SCF ) distributions\n",
      "approach\n",
      "\n",
      "\n",
      "1219\n",
      "The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .\n",
      "post-processing\n",
      "error correction\n",
      "\n",
      "\n",
      "1244\n",
      "The representation contains complementary information to that learned from supervised image datasets like ImageNet .\n",
      "complementary information\n",
      "representation\n",
      "\n",
      "\n",
      "1257\n",
      "Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into speech synthesis systems , in particular , concept-to-speech systems , is also briefly discussed .\n",
      "discourse-level construct\n",
      "speech synthesis systems\n",
      "\n",
      "\n",
      "1259\n",
      "Inference in these models involves solving a combinatorial optimization problem , with methods such as graph cuts , belief propagation .\n",
      "combinatorial optimization problem\n",
      "Inference\n",
      "\n",
      "\n",
      "1271\n",
      "Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .\n",
      "recognition network\n",
      "generalized metaphor\n",
      "\n",
      "\n",
      "1271\n",
      "Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .\n",
      "basic mapping\n",
      "generalized metaphor\n",
      "\n",
      "\n",
      "1271\n",
      "Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .\n",
      "transfer mappings\n",
      "generalized metaphor\n",
      "\n",
      "\n",
      "1271\n",
      "Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .\n",
      "implicit intention component\n",
      "generalized metaphor\n",
      "\n",
      "\n",
      "1286\n",
      "The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building natural language processing ( NLP ) systems for text understanding .\n",
      "core English lexicon\n",
      "system\n",
      "\n",
      "\n",
      "1286\n",
      "The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building natural language processing ( NLP ) systems for text understanding .\n",
      "grammar\n",
      "system\n",
      "\n",
      "\n",
      "1286\n",
      "The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building natural language processing ( NLP ) systems for text understanding .\n",
      "concept representations\n",
      "system\n",
      "\n",
      "\n",
      "1298\n",
      "The co-occurrence pattern , a combination of binary or local features , is more discriminative than individual features and has shown its advantages in object , scene , and action recognition .\n",
      "binary or local features\n",
      "co-occurrence pattern\n",
      "\n",
      "\n",
      "1302\n",
      "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .\n",
      "AND and OR patterns\n",
      "boosting\n",
      "\n",
      "\n",
      "1321\n",
      "This system consists of one or more reference times and temporal perspective times , the speech time and the location time .\n",
      "reference times\n",
      "system\n",
      "\n",
      "\n",
      "1321\n",
      "This system consists of one or more reference times and temporal perspective times , the speech time and the location time .\n",
      "temporal perspective times\n",
      "system\n",
      "\n",
      "\n",
      "1321\n",
      "This system consists of one or more reference times and temporal perspective times , the speech time and the location time .\n",
      "speech time\n",
      "system\n",
      "\n",
      "\n",
      "1321\n",
      "This system consists of one or more reference times and temporal perspective times , the speech time and the location time .\n",
      "location time\n",
      "system\n",
      "\n",
      "\n",
      "1325\n",
      "The work presented in this paper is the first step in a project which aims to cluster and summarise electronic discussions in the context of help-desk applications .\n",
      "electronic discussions\n",
      "help-desk applications\n",
      "\n",
      "\n",
      "1340\n",
      "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach .\n",
      "knowledge-based techniques\n",
      "hybrid system\n",
      "\n",
      "\n",
      "1340\n",
      "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach .\n",
      "corpus-based approach\n",
      "hybrid system\n",
      "\n",
      "\n",
      "1358\n",
      "Spelling-checkers have become an integral part of most text processing software .\n",
      "Spelling-checkers\n",
      "text processing software\n",
      "\n",
      "\n",
      "1378\n",
      "We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems .\n",
      "user modeling\n",
      "spoken dialogue systems\n",
      "\n",
      "\n",
      "1393\n",
      "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .\n",
      "paraphrases\n",
      "bilingual parallel corpus\n",
      "\n",
      "\n",
      "1394\n",
      "We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments .\n",
      "paraphrases\n",
      "automatic alignments\n",
      "\n",
      "\n",
      "1395\n",
      "This paper proposes an automatic , essentially domain-independent means of evaluating Spoken Language Systems ( SLS ) which combines software we have developed for that purpose ( the '' Comparator '' ) and a set of specifications for answer expressions ( the '' Common Answer Specification '' , or CAS ) .\n",
      "software\n",
      "domain-independent means of evaluating Spoken Language Systems ( SLS )\n",
      "\n",
      "\n",
      "1395\n",
      "This paper proposes an automatic , essentially domain-independent means of evaluating Spoken Language Systems ( SLS ) which combines software we have developed for that purpose ( the '' Comparator '' ) and a set of specifications for answer expressions ( the '' Common Answer Specification '' , or CAS ) .\n",
      "specifications\n",
      "domain-independent means of evaluating Spoken Language Systems ( SLS )\n",
      "\n",
      "\n",
      "1412\n",
      "This has given rise to discussions about the relative placement of these new modules in the overall architecture .\n",
      "modules\n",
      "overall architecture\n",
      "\n",
      "\n",
      "1414\n",
      "We present examples which suggest that in a pipelined NLG architecture , the best approach is to strongly tie it to a revision component .\n",
      "revision component\n",
      "pipelined NLG architecture\n",
      "\n",
      "\n",
      "1417\n",
      "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .\n",
      "components\n",
      "system\n",
      "\n",
      "\n",
      "1417\n",
      "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .\n",
      "syntactic analyzer\n",
      "components\n",
      "\n",
      "\n",
      "1417\n",
      "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .\n",
      "semantic analyzer\n",
      "components\n",
      "\n",
      "\n",
      "1417\n",
      "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .\n",
      "dictionary\n",
      "components\n",
      "\n",
      "\n",
      "1424\n",
      "The method combined the log-likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .\n",
      "log-likelihood\n",
      "method\n",
      "\n",
      "\n",
      "1428\n",
      "We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear ( maximum-entropy ) models .\n",
      "feature selection methods\n",
      "log-linear ( maximum-entropy ) models\n",
      "\n",
      "\n",
      "1458\n",
      "The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph .\n",
      "translation paths\n",
      "graph\n",
      "\n",
      "\n",
      "1459\n",
      "We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word-lemma based feature function models produce better results than token-based models , 2 ) adding a PoS-tag feature function to the word-lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated .\n",
      "PoS-tag feature function\n",
      "word-lemma model\n",
      "\n",
      "\n",
      "1475\n",
      "We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .\n",
      "analysis\n",
      "diagnostic tool\n",
      "\n",
      "\n",
      "1492\n",
      "The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems .\n",
      "system response\n",
      "natural language generation community\n",
      "\n",
      "\n",
      "1548\n",
      "Motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new similarity measures and expanded stop lists .\n",
      "part of speech tagging\n",
      "performance enhancing techniques\n",
      "\n",
      "\n",
      "1548\n",
      "Motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new similarity measures and expanded stop lists .\n",
      "similarity measures\n",
      "performance enhancing techniques\n",
      "\n",
      "\n",
      "1548\n",
      "Motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new similarity measures and expanded stop lists .\n",
      "expanded stop lists\n",
      "performance enhancing techniques\n",
      "\n",
      "\n",
      "1558\n",
      "In this paper , we present a corpus-based supervised word sense disambiguation ( WSD ) system for Dutch which combines statistical classification ( maximum entropy ) with linguistic information .\n",
      "statistical classification\n",
      "corpus-based supervised word sense disambiguation ( WSD ) system\n",
      "\n",
      "\n",
      "1558\n",
      "In this paper , we present a corpus-based supervised word sense disambiguation ( WSD ) system for Dutch which combines statistical classification ( maximum entropy ) with linguistic information .\n",
      "maximum entropy\n",
      "corpus-based supervised word sense disambiguation ( WSD ) system\n",
      "\n",
      "\n",
      "1558\n",
      "In this paper , we present a corpus-based supervised word sense disambiguation ( WSD ) system for Dutch which combines statistical classification ( maximum entropy ) with linguistic information .\n",
      "linguistic information\n",
      "corpus-based supervised word sense disambiguation ( WSD ) system\n",
      "\n",
      "\n",
      "1575\n",
      "The proposed mechanism includes title-driven name recognition , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title .\n",
      "title-driven name recognition\n",
      "mechanism\n",
      "\n",
      "\n",
      "1575\n",
      "The proposed mechanism includes title-driven name recognition , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title .\n",
      "adaptive dynamic word formation\n",
      "mechanism\n",
      "\n",
      "\n",
      "1575\n",
      "The proposed mechanism includes title-driven name recognition , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title .\n",
      "identification of 2-character and 3-character Chinese names without title\n",
      "mechanism\n",
      "\n",
      "\n",
      "1625\n",
      "While such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures .\n",
      "non-literal aspects of communication\n",
      "they\n",
      "\n",
      "\n",
      "1626\n",
      "This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects , while still satisfying human communication needs .\n",
      "graphics displays\n",
      "personal computers\n",
      "\n",
      "\n",
      "1637\n",
      "Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT .\n",
      "problems\n",
      "SMT\n",
      "\n",
      "\n",
      "1734\n",
      "We develop a new model , TSI-pLSA , which extends pLSA ( as applied to visual words ) to include spatial information in a translation and scale invariant manner .\n",
      "spatial information\n",
      "TSI-pLSA\n",
      "\n",
      "\n",
      "1742\n",
      "In this paper , we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system .\n",
      "pronominal anaphora resolution module\n",
      "Lucy\n",
      "\n",
      "\n",
      "1770\n",
      "The transfer phase in machine translation ( MT ) systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .\n",
      "transfer phase\n",
      "machine translation ( MT ) systems\n",
      "\n",
      "\n",
      "1787\n",
      "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system .\n",
      "transfer\n",
      "system\n",
      "\n",
      "\n",
      "1787\n",
      "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system .\n",
      "direct approaches\n",
      "system\n",
      "\n",
      "\n",
      "1802\n",
      "We propose a process model for hierarchical perceptual sound organization , which recognizes perceptual sounds included in incoming sound signals .\n",
      "perceptual sounds\n",
      "incoming sound signals\n",
      "\n",
      "\n",
      "1804\n",
      "Our model consists of multiple processing modules and a hypothesis network for quantitative integration of multiple sources of information .\n",
      "processing modules\n",
      "model\n",
      "\n",
      "\n",
      "1804\n",
      "Our model consists of multiple processing modules and a hypothesis network for quantitative integration of multiple sources of information .\n",
      "hypothesis network\n",
      "model\n",
      "\n",
      "\n",
      "1806\n",
      "On the hypothesis network , individual information is integrated and an optimal internal model of perceptual sounds is automatically constructed .\n",
      "internal model\n",
      "hypothesis network\n",
      "\n",
      "\n",
      "1847\n",
      "In this paper , we want to show how the morphological component of an existing NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system ( Linguistic String Project - Medical Language Processor ) of the New York University .\n",
      "morphological component\n",
      "NLP-system for Dutch ( Dutch Medical Language Processor - DMLP )\n",
      "\n",
      "\n",
      "1847\n",
      "In this paper , we want to show how the morphological component of an existing NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system ( Linguistic String Project - Medical Language Processor ) of the New York University .\n",
      "language independent modules\n",
      "LSP-MLP system ( Linguistic String Project - Medical Language Processor )\n",
      "\n",
      "\n",
      "1849\n",
      "This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a patient discharge summary ( PDS ) by means of modern HyperText Mark-Up Language ( HTML ) technology .\n",
      "relevant information\n",
      "patient discharge summary ( PDS )\n",
      "\n",
      "\n",
      "1851\n",
      "CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements ( e.g. , thesis statements ) .\n",
      "essay-based discourse elements\n",
      "CriterionSM Online Essay Evaluation Service\n",
      "\n",
      "\n",
      "1857\n",
      "This paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .\n",
      "CURVE-ELEMENT tokens\n",
      "spatially-indexed and scale-indexed data structure\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_c = 0\n",
    "pair_dist = []\n",
    "pair_type = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Part-of']:\n",
    "                    \n",
    "            hs, he, ht = entities[rel['head']]['start'], entities[rel['head']]['end'], entities[rel['head']]['type']\n",
    "            ts, te, tt = entities[rel['tail']]['start'], entities[rel['tail']]['end'], entities[rel['tail']]['type']\n",
    "            \n",
    "            print(z)\n",
    "            print(' '.join(row['tokens']))\n",
    "            print(' '.join(row['tokens'][hs:he]))\n",
    "            print(' '.join(row['tokens'][ts:te]))\n",
    "            count += 1\n",
    "            if (he-hs) > (te-ts):\n",
    "                count_c += 1\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3624161073825503"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_c/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_exist(head, tail, rtype, relations):\n",
    "    if {\n",
    "        'head': head,\n",
    "        'tail': tail,\n",
    "        'type': rtype\n",
    "    } in relations:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 {'tokens': ['We', 'show', 'that', 'SHORTSTR2', 'is', 'complementary', 'to', 'the', 'existing', 'algorithms', 'SHORTGAC', 'and', 'HAGGISGAC', 'that', 'exploit', 'short', 'supports', ',', 'while', 'being', 'much', 'simpler', '.'], 'entities': [{'type': 'Method', 'start': 3, 'end': 4}, {'type': 'Method', 'start': 10, 'end': 11}, {'type': 'Method', 'start': 12, 'end': 13}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'f90c7140-6d14-4a76-8a7b-46d39f0ef2b0'}\n",
      "95 {'tokens': ['We', 'show', 'that', 'SHORTSTR2', 'is', 'complementary', 'to', 'the', 'existing', 'algorithms', 'SHORTGAC', 'and', 'HAGGISGAC', 'that', 'exploit', 'short', 'supports', ',', 'while', 'being', 'much', 'simpler', '.'], 'entities': [{'type': 'Method', 'start': 3, 'end': 4}, {'type': 'Method', 'start': 10, 'end': 11}, {'type': 'Method', 'start': 12, 'end': 13}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'f90c7140-6d14-4a76-8a7b-46d39f0ef2b0'}\n",
      "162 {'tokens': ['The', 'model', 'gives', 'an', 'F-measure', 'improvement', 'of', '~', '1.25', '%', 'beyond', 'the', 'base', 'parser', ',', 'and', 'an', '~', '0.25', '%', 'improvement', 'beyond', 'Collins', '(', '2000', ')', 'reranker', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 2}, {'type': 'Metric', 'start': 4, 'end': 5}, {'type': 'Method', 'start': 12, 'end': 14}, {'type': 'Method', 'start': 22, 'end': 27}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': 'f181f329-1750-49e0-af68-dba662735fdf'}\n",
      "162 {'tokens': ['The', 'model', 'gives', 'an', 'F-measure', 'improvement', 'of', '~', '1.25', '%', 'beyond', 'the', 'base', 'parser', ',', 'and', 'an', '~', '0.25', '%', 'improvement', 'beyond', 'Collins', '(', '2000', ')', 'reranker', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 2}, {'type': 'Metric', 'start': 4, 'end': 5}, {'type': 'Method', 'start': 12, 'end': 14}, {'type': 'Method', 'start': 22, 'end': 27}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': 'f181f329-1750-49e0-af68-dba662735fdf'}\n",
      "188 {'tokens': ['Experimental', 'results', 'on', 'the', 'Olympic', 'Sports', 'and', 'UCF101', 'datasets', 'demonstrate', 'that', 'the', 'proposed', 'attribute-based', 'representation', 'can', 'significantly', 'boost', 'the', 'performance', 'of', 'action', 'recognition', 'algorithms', 'and', 'outperform', 'most', 'recently', 'proposed', 'recognition', 'approaches', '.'], 'entities': [{'type': 'Material', 'start': 4, 'end': 9}, {'type': 'Method', 'start': 13, 'end': 15}, {'type': 'Method', 'start': 21, 'end': 24}, {'type': 'Method', 'start': 29, 'end': 31}], 'relations': [{'type': 'Evaluate-for', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': '11bc49cc-9aa0-45fa-881c-322cfa4d39ef'}\n",
      "198 {'tokens': ['Contrary', 'to', 'existing', 'greedy', 'techniques', ',', 'these', 'tasks', 'are', 'posed', 'in', 'the', 'form', 'of', 'a', 'discrete', 'global', 'optimization', 'problem', 'with', 'a', 'well', 'defined', 'objective', 'function', '.'], 'entities': [{'type': 'Method', 'start': 3, 'end': 5}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Task', 'start': 15, 'end': 19}, {'type': 'OtherScientificTerm', 'start': 21, 'end': 25}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Feature-of', 'head': 2, 'tail': 1}, {'type': 'Feature-of', 'head': 3, 'tail': 2}], 'orig_id': '7f796922-eaee-4052-b83e-64ff724a3b2d'}\n",
      "205 {'tokens': ['Over', 'two', 'distinct', 'datasets', ',', 'we', 'find', 'that', 'indexing', 'according', 'to', 'simple', 'character', 'bigrams', 'produces', 'a', 'retrieval', 'accuracy', 'superior', 'to', 'any', 'of', 'the', 'tested', 'word', 'N-gram', 'models', '.'], 'entities': [{'type': 'Task', 'start': 8, 'end': 9}, {'type': 'Method', 'start': 12, 'end': 14}, {'type': 'Metric', 'start': 16, 'end': 18}, {'type': 'Method', 'start': 24, 'end': 27}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Evaluate-for', 'head': 2, 'tail': 1}, {'type': 'Evaluate-for', 'head': 2, 'tail': 3}], 'orig_id': '8c658706-8b53-4b73-906f-7810bdb68699'}\n",
      "214 {'tokens': ['First', ',', 'instead', 'of', 'using', 'the', 'model', 'space', 'as', 'a', 'regular-izer', ',', 'we', 'directly', 'use', 'it', 'as', 'our', 'search', 'space', ',', 'thus', 'resulting', 'in', 'a', 'more', 'elegant', 'formulation', 'with', 'fewer', 'unknowns', 'and', 'fewer', 'equations', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 6, 'end': 8}, {'type': 'OtherScientificTerm', 'start': 10, 'end': 11}, {'type': 'Generic', 'start': 15, 'end': 16}, {'type': 'OtherScientificTerm', 'start': 18, 'end': 20}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 3}], 'orig_id': '22af0aaf-c0fe-435d-899f-2d4bf50bab0c'}\n",
      "219 {'tokens': ['While', 'the', 'model', 'is', 'more', 'complex', 'than', 'those', 'which', 'have', 'been', 'employed', 'for', 'unsupervised', 'learning', 'of', 'POS', 'tags', 'in', 'English', ',', 'which', 'use', 'only', 'syntactic', 'information', ',', 'the', 'variety', 'of', 'languages', 'in', 'the', 'world', 'requires', 'that', 'we', 'consider', 'morphology', 'as', 'well', '.'], 'entities': [{'type': 'Generic', 'start': 2, 'end': 3}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Task', 'start': 13, 'end': 20}, {'type': 'OtherScientificTerm', 'start': 24, 'end': 26}, {'type': 'OtherScientificTerm', 'start': 38, 'end': 39}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 3, 'tail': 1}], 'orig_id': 'c9f042e1-4c62-4b5d-9385-23c664274267'}\n",
      "230 {'tokens': ['The', 'accuracy', 'of', 'the', 'statistical', 'method', 'is', 'reasonably', 'good', ',', 'comparable', 'to', 'taggers', 'for', 'English', '.'], 'entities': [{'type': 'Metric', 'start': 1, 'end': 2}, {'type': 'Method', 'start': 4, 'end': 6}, {'type': 'Method', 'start': 12, 'end': 13}, {'type': 'Material', 'start': 14, 'end': 15}], 'relations': [{'type': 'Evaluate-for', 'head': 0, 'tail': 1}, {'type': 'Evaluate-for', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 2, 'tail': 3}], 'orig_id': '7648663a-ffab-45e2-85cd-3b9308d6c7f7'}\n",
      "244 {'tokens': ['It', 'also', 'facilitates', 'more', 'efficient', 'statistical', 'ranking', 'than', 'a', 'previous', 'approach', 'to', 'statistical', 'generation', '.'], 'entities': [{'type': 'Generic', 'start': 0, 'end': 1}, {'type': 'Method', 'start': 5, 'end': 7}, {'type': 'Generic', 'start': 10, 'end': 11}, {'type': 'Task', 'start': 12, 'end': 14}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Used-for', 'head': 2, 'tail': 3}], 'orig_id': '9230de0d-e782-48e3-8b0f-b311741761bb'}\n",
      "245 {'tokens': ['An', 'efficient', 'ranking', 'algorithm', 'is', 'described', ',', 'together', 'with', 'experimental', 'results', 'showing', 'significant', 'improvements', 'over', 'simple', 'enumeration', 'or', 'a', 'lattice-based', 'approach', '.'], 'entities': [{'type': 'Method', 'start': 2, 'end': 4}, {'type': 'Method', 'start': 16, 'end': 17}, {'type': 'Method', 'start': 19, 'end': 21}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'f324bc8b-ecf4-46cf-bb95-bca8826de1f5'}\n",
      "245 {'tokens': ['An', 'efficient', 'ranking', 'algorithm', 'is', 'described', ',', 'together', 'with', 'experimental', 'results', 'showing', 'significant', 'improvements', 'over', 'simple', 'enumeration', 'or', 'a', 'lattice-based', 'approach', '.'], 'entities': [{'type': 'Method', 'start': 2, 'end': 4}, {'type': 'Method', 'start': 16, 'end': 17}, {'type': 'Method', 'start': 19, 'end': 21}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'f324bc8b-ecf4-46cf-bb95-bca8826de1f5'}\n",
      "249 {'tokens': ['For', 'this', 'purpose', ',', 'we', 'have', 'designed', 'a', 'version', 'of', 'KL-ONE', 'which', 'represents', 'the', 'epistemological', 'level', ',', 'while', 'the', 'new', 'experimental', 'language', ',', 'KL-Conc', ',', 'represents', 'the', 'conceptual', 'level', '.'], 'entities': [{'type': 'Method', 'start': 10, 'end': 11}, {'type': 'OtherScientificTerm', 'start': 14, 'end': 16}, {'type': 'Generic', 'start': 20, 'end': 22}, {'type': 'Method', 'start': 23, 'end': 24}, {'type': 'OtherScientificTerm', 'start': 27, 'end': 29}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Feature-of', 'head': 1, 'tail': 0}, {'type': 'Feature-of', 'head': 4, 'tail': 3}], 'orig_id': 'd5e38261-66cd-44c3-8d2d-18842add907e'}\n",
      "318 {'tokens': ['In', 'experiments', 'using', 'the', 'Penn', 'WSJ', 'corpus', ',', 'our', 'automatically', 'trained', 'model', 'gave', 'a', 'performance', 'of', '86.6', '%', '(', 'F1', ',', 'sentences', '<', '40', 'words', ')', ',', 'which', 'is', 'comparable', 'to', 'that', 'of', 'an', 'unlexicalized', 'PCFG', 'parser', 'created', 'using', 'extensive', 'manual', 'feature', 'selection', '.'], 'entities': [{'type': 'Material', 'start': 4, 'end': 7}, {'type': 'Generic', 'start': 11, 'end': 12}, {'type': 'Metric', 'start': 19, 'end': 20}, {'type': 'Method', 'start': 34, 'end': 37}, {'type': 'Method', 'start': 40, 'end': 43}], 'relations': [{'type': 'Evaluate-for', 'head': 0, 'tail': 1}, {'type': 'Evaluate-for', 'head': 0, 'tail': 3}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Evaluate-for', 'head': 2, 'tail': 1}, {'type': 'Evaluate-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 4, 'tail': 3}], 'orig_id': 'eb84598b-b0f4-445a-999e-06a118651d18'}\n",
      "338 {'tokens': ['We', 'demonstrate', 'that', 'an', 'approximation', 'of', 'HPSG', 'produces', 'a', 'more', 'effective', 'CFG', 'filter', 'than', 'that', 'of', 'LTAG', '.'], 'entities': [{'type': 'Method', 'start': 4, 'end': 7}, {'type': 'Method', 'start': 6, 'end': 7}, {'type': 'Method', 'start': 11, 'end': 13}, {'type': 'Generic', 'start': 14, 'end': 15}, {'type': 'Method', 'start': 16, 'end': 17}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 3, 'tail': 4}], 'orig_id': 'b07f970c-cba6-4ae5-8a60-9d02e6a8b9b5'}\n",
      "347 {'tokens': ['The', 'result', 'shows', 'that', 'our', 'system', 'outperforms', 'the', 'baseline', 'system', 'based', 'on', 'the', 'IBM', 'models', 'in', 'both', 'translation', 'speed', 'and', 'quality', '.'], 'entities': [{'type': 'Generic', 'start': 5, 'end': 6}, {'type': 'Generic', 'start': 8, 'end': 10}, {'type': 'Method', 'start': 13, 'end': 15}, {'type': 'Metric', 'start': 17, 'end': 21}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 2, 'tail': 1}, {'type': 'Evaluate-for', 'head': 3, 'tail': 0}, {'type': 'Evaluate-for', 'head': 3, 'tail': 1}], 'orig_id': '39a708f2-13a5-47fb-963e-c61ae92fb656'}\n",
      "365 {'tokens': ['We', 'show', 'that', 'AS', 'is', 'a', 'particular', 'instance', 'of', 'the', 'Ant-Q', 'family', ',', 'and', 'that', 'there', 'are', 'instances', 'of', 'this', 'family', 'which', 'perform', 'better', 'than', 'AS', '.'], 'entities': [{'type': 'Method', 'start': 3, 'end': 4}, {'type': 'Method', 'start': 10, 'end': 12}, {'type': 'Generic', 'start': 17, 'end': 18}, {'type': 'Generic', 'start': 20, 'end': 21}, {'type': 'OtherScientificTerm', 'start': 25, 'end': 26}], 'relations': [{'type': 'Hyponym-of', 'head': 0, 'tail': 1}, {'type': 'Part-of', 'head': 2, 'tail': 3}, {'type': 'Compare', 'head': 2, 'tail': 4}], 'orig_id': '27df7bd3-bd70-45d4-bb31-00a65320e4a2'}\n",
      "366 {'tokens': ['We', 'experimentally', 'investigate', 'the', 'functioning', 'of', 'Ant-Q', 'and', 'we', 'show', 'that', 'the', 'results', 'obtained', 'by', 'Ant-Q', 'on', 'symmetric', 'TSP', \"'s\", 'are', 'competitive', 'with', 'those', 'obtained', 'by', 'other', 'heuristic', 'approaches', 'based', 'on', 'neural', 'networks', 'or', 'local', 'search', '.'], 'entities': [{'type': 'Method', 'start': 6, 'end': 7}, {'type': 'Method', 'start': 15, 'end': 16}, {'type': 'Task', 'start': 17, 'end': 19}, {'type': 'Method', 'start': 27, 'end': 29}, {'type': 'Method', 'start': 31, 'end': 33}, {'type': 'Method', 'start': 34, 'end': 36}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Used-for', 'head': 4, 'tail': 3}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Used-for', 'head': 5, 'tail': 3}], 'orig_id': '68ff55d9-0f84-46fd-a677-9dee641eac8d'}\n",
      "379 {'tokens': ['The', 'development', 'and', 'evolution', 'of', 'such', 'a', 'hybrid', 'architecture', 'has', 'lead', 'to', 'a', 'parser', 'which', 'is', 'superior', 'to', 'any', 'known', 'deterministic', 'parser', '.'], 'entities': [{'type': 'Method', 'start': 7, 'end': 9}, {'type': 'Method', 'start': 13, 'end': 14}, {'type': 'Method', 'start': 20, 'end': 22}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': 'f70be101-ad52-4f29-aeb0-49c57e28ca0e'}\n",
      "395 {'tokens': ['The', 'results', 'demonstrates', 'that', 'the', 'classifier', 'based', 'on', 'SAE', 'detects', 'the', 'ASR', 'errors', 'better', 'than', 'the', 'other', 'classification', 'methods', '.'], 'entities': [{'type': 'Method', 'start': 5, 'end': 6}, {'type': 'Method', 'start': 8, 'end': 9}, {'type': 'OtherScientificTerm', 'start': 11, 'end': 13}, {'type': 'Method', 'start': 17, 'end': 19}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 3, 'tail': 2}], 'orig_id': '9d7ae30c-3879-4f05-a878-6e2cdf402b4d'}\n",
      "408 {'tokens': ['The', 'results', 'of', 'the', 'experiments', 'demonstrate', 'that', 'the', 'HDAG', 'Kernel', 'is', 'superior', 'to', 'other', 'kernel', 'functions', 'and', 'baseline', 'methods', '.'], 'entities': [{'type': 'Method', 'start': 8, 'end': 10}, {'type': 'Method', 'start': 14, 'end': 16}, {'type': 'Generic', 'start': 17, 'end': 19}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'e04bc680-ccde-405f-ada3-6d692bd515aa'}\n",
      "408 {'tokens': ['The', 'results', 'of', 'the', 'experiments', 'demonstrate', 'that', 'the', 'HDAG', 'Kernel', 'is', 'superior', 'to', 'other', 'kernel', 'functions', 'and', 'baseline', 'methods', '.'], 'entities': [{'type': 'Method', 'start': 8, 'end': 10}, {'type': 'Method', 'start': 14, 'end': 16}, {'type': 'Generic', 'start': 17, 'end': 19}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Conjunction', 'head': 1, 'tail': 2}], 'orig_id': 'e04bc680-ccde-405f-ada3-6d692bd515aa'}\n",
      "418 {'tokens': ['Our', 'experiment', 'result', 'shows', 'that', 'the', 'neural', 'network', 'can', 'learn', 'a', 'language', 'model', 'that', 'has', 'performance', 'even', 'better', 'than', 'standard', 'statistical', 'methods', '.'], 'entities': [{'type': 'Method', 'start': 6, 'end': 8}, {'type': 'Method', 'start': 11, 'end': 13}, {'type': 'Method', 'start': 20, 'end': 22}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}], 'orig_id': '4bdd0738-2ed9-41f4-9b03-c4739323fbe3'}\n",
      "429 {'tokens': ['We', 'propose', 'and', 'analyze', 'a', 'block', 'minimization', 'framework', 'for', 'data', 'larger', 'than', 'the', 'memory', 'size', '.'], 'entities': [{'type': 'Method', 'start': 5, 'end': 8}, {'type': 'Generic', 'start': 9, 'end': 10}, {'type': 'OtherScientificTerm', 'start': 13, 'end': 15}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': 'b8ee7ef4-f494-4d77-bcd1-372c1fa09cfc'}\n",
      "467 {'tokens': ['Our', 'resource-frugal', 'approach', 'results', 'in', '87.5', '%', 'agreement', 'with', 'a', 'state', 'of', 'the', 'art', ',', 'proprietary', 'Arabic', 'stemmer', 'built', 'using', 'rules', ',', 'affix', 'lists', ',', 'and', 'human', 'annotated', 'text', ',', 'in', 'addition', 'to', 'an', 'unsupervised', 'component', '.'], 'entities': [{'type': 'Method', 'start': 1, 'end': 3}, {'type': 'Metric', 'start': 7, 'end': 8}, {'type': 'Method', 'start': 16, 'end': 18}, {'type': 'OtherScientificTerm', 'start': 20, 'end': 21}, {'type': 'Material', 'start': 22, 'end': 24}, {'type': 'Material', 'start': 26, 'end': 29}, {'type': 'Method', 'start': 34, 'end': 36}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Evaluate-for', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 3, 'tail': 2}, {'type': 'Conjunction', 'head': 3, 'tail': 4}, {'type': 'Used-for', 'head': 4, 'tail': 2}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Used-for', 'head': 5, 'tail': 2}, {'type': 'Conjunction', 'head': 5, 'tail': 6}, {'type': 'Used-for', 'head': 6, 'tail': 2}], 'orig_id': 'b0565840-2303-4e8a-bcaf-d5dad4d6af54'}\n",
      "487 {'tokens': ['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'phrase-based', 'unigram', 'model', 'for', 'statistical', 'machine', 'translation', 'that', 'uses', 'a', 'much', 'simpler', 'set', 'of', 'model', 'parameters', 'than', 'similar', 'phrase-based', 'models', '.'], 'entities': [{'type': 'Method', 'start': 7, 'end': 10}, {'type': 'Task', 'start': 11, 'end': 14}, {'type': 'OtherScientificTerm', 'start': 21, 'end': 23}, {'type': 'Method', 'start': 25, 'end': 27}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 0}], 'orig_id': '1d9ced45-332b-4429-b331-d5b72491a42c'}\n",
      "590 {'tokens': ['A', 'further', 'reduction', 'in', 'the', 'search', 'space', 'is', 'achieved', 'by', 'using', 'semantic', 'rather', 'than', 'syntactic', 'categories', 'on', 'the', 'terminal', 'and', 'non-terminal', 'edges', ',', 'thereby', 'reducing', 'the', 'amount', 'of', 'ambiguity', 'and', 'thus', 'the', 'number', 'of', 'edges', ',', 'since', 'only', 'edges', 'with', 'a', 'valid', 'semantic', 'interpretation', 'are', 'ever', 'introduced', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 2, 'end': 7}, {'type': 'OtherScientificTerm', 'start': 11, 'end': 12}, {'type': 'OtherScientificTerm', 'start': 14, 'end': 16}, {'type': 'OtherScientificTerm', 'start': 18, 'end': 22}, {'type': 'OtherScientificTerm', 'start': 34, 'end': 35}, {'type': 'OtherScientificTerm', 'start': 38, 'end': 39}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Feature-of', 'head': 1, 'tail': 3}, {'type': 'Feature-of', 'head': 2, 'tail': 3}], 'orig_id': '6351e6f2-7454-4bbf-af5e-1889d71c8faf'}\n",
      "643 {'tokens': ['The', 'speech-search', 'algorithm', 'is', 'implemented', 'on', 'a', 'board', 'with', 'a', 'single', 'Intel', 'i860', 'chip', ',', 'which', 'provides', 'a', 'factor', 'of', '5', 'speed-up', 'over', 'a', 'SUN', '4', 'for', 'straight', 'C', 'code', '.'], 'entities': [{'type': 'Method', 'start': 1, 'end': 3}, {'type': 'OtherScientificTerm', 'start': 7, 'end': 8}, {'type': 'OtherScientificTerm', 'start': 11, 'end': 14}, {'type': 'OtherScientificTerm', 'start': 24, 'end': 26}, {'type': 'OtherScientificTerm', 'start': 27, 'end': 30}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Part-of', 'head': 2, 'tail': 1}, {'type': 'Compare', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 4}, {'type': 'Used-for', 'head': 3, 'tail': 4}], 'orig_id': 'a0f454c2-57d7-4b5e-840b-5aa01da8ecb9'}\n",
      "677 {'tokens': ['The', 'method', 'overcomes', 'the', 'limitations', 'of', 'conventional', 'statistical', 'methods', 'which', 'require', 'large', 'corpora', 'to', 'be', 'effective', ',', 'and', 'lexical', 'approaches', 'which', 'depend', 'on', 'existing', 'bilingual', 'dictionaries', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 2}, {'type': 'Method', 'start': 7, 'end': 9}, {'type': 'Material', 'start': 11, 'end': 13}, {'type': 'Method', 'start': 18, 'end': 20}, {'type': 'Material', 'start': 24, 'end': 26}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 1}, {'type': 'Used-for', 'head': 4, 'tail': 3}], 'orig_id': '15af7614-7040-4a82-bb9a-fa9e1761d603'}\n",
      "677 {'tokens': ['The', 'method', 'overcomes', 'the', 'limitations', 'of', 'conventional', 'statistical', 'methods', 'which', 'require', 'large', 'corpora', 'to', 'be', 'effective', ',', 'and', 'lexical', 'approaches', 'which', 'depend', 'on', 'existing', 'bilingual', 'dictionaries', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 2}, {'type': 'Method', 'start': 7, 'end': 9}, {'type': 'Material', 'start': 11, 'end': 13}, {'type': 'Method', 'start': 18, 'end': 20}, {'type': 'Material', 'start': 24, 'end': 26}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 1}, {'type': 'Used-for', 'head': 4, 'tail': 3}], 'orig_id': '15af7614-7040-4a82-bb9a-fa9e1761d603'}\n",
      "748 {'tokens': ['In', 'contrast', 'to', 'existing', 'methods', 'that', 'consider', 'only', 'the', 'guidance', 'image', ',', 'our', 'method', 'can', 'selectively', 'transfer', 'salient', 'structures', 'that', 'are', 'consistent', 'in', 'both', 'guidance', 'and', 'target', 'images', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Material', 'start': 9, 'end': 11}, {'type': 'Generic', 'start': 13, 'end': 14}, {'type': 'Task', 'start': 16, 'end': 19}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 2, 'tail': 3}], 'orig_id': '797e4add-0ac4-4652-87c4-589cd884c524'}\n",
      "845 {'tokens': ['We', 'show', 'that', 'the', 'trained', 'SPR', 'learns', 'to', 'select', 'a', 'sentence', 'plan', 'whose', 'rating', 'on', 'average', 'is', 'only', '5', '%', 'worse', 'than', 'the', 'top', 'human-ranked', 'sentence', 'plan', '.'], 'entities': [{'type': 'Method', 'start': 5, 'end': 6}, {'type': 'OtherScientificTerm', 'start': 10, 'end': 12}, {'type': 'OtherScientificTerm', 'start': 23, 'end': 27}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': '5af3f65c-28f2-422a-a295-e43255827641'}\n",
      "891 {'tokens': ['Understanding', 'natural', 'images', 'has', 'been', 'extensively', 'studied', 'in', 'computer', 'vision', ',', 'while', 'diagram', 'understanding', 'has', 'received', 'little', 'attention', '.'], 'entities': [{'type': 'Task', 'start': 0, 'end': 3}, {'type': 'Material', 'start': 1, 'end': 3}, {'type': 'Task', 'start': 8, 'end': 10}, {'type': 'Task', 'start': 12, 'end': 14}], 'relations': [{'type': 'Part-of', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 0, 'tail': 3}], 'orig_id': 'd954cf77-f12e-432a-b9c2-39b1d51f6124'}\n",
      "906 {'tokens': ['Extensive', 'experiments', 'show', 'the', 'superior', 'performance', 'of', 'our', 'approach', 'over', 'state-of-the-art', 'change', 'detection', 'methods', 'and', 'its', 'ability', 'to', 'distinguish', 'real', 'scene', 'changes', 'from', 'false', 'ones', 'caused', 'by', 'lighting', 'variations', '.'], 'entities': [{'type': 'Generic', 'start': 8, 'end': 9}, {'type': 'Method', 'start': 11, 'end': 14}, {'type': 'OtherScientificTerm', 'start': 19, 'end': 22}, {'type': 'OtherScientificTerm', 'start': 27, 'end': 29}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 0, 'tail': 2}], 'orig_id': '92a23096-4f9a-4d4a-b1a0-0996472e14b1'}\n",
      "918 {'tokens': ['The', 'experimental', 'results', 'demonstrate', 'that', 'the', 'proposed', 'method', 'makes', 'it', 'possible', 'to', 'significantly', 'improve', 'speech', 'quality', 'in', 'the', 'converted', 'singing', 'voice', 'while', 'preserving', 'the', 'conversion', 'accuracy', 'of', 'singer', 'identity', 'compared', 'to', 'the', 'conventional', 'SVC', '.'], 'entities': [{'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Metric', 'start': 14, 'end': 16}, {'type': 'Metric', 'start': 24, 'end': 29}, {'type': 'Method', 'start': 33, 'end': 34}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Evaluate-for', 'head': 1, 'tail': 3}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Used-for', 'head': 2, 'tail': 3}], 'orig_id': '4c259a2f-da56-4893-b39f-d2e832cba7cf'}\n",
      "924 {'tokens': ['Unlike', 'evaluations', 'in', 'the', 'SRE', 'series', ',', 'the', 'i-vector', 'challenge', 'was', 'run', 'entirely', 'online', 'and', 'used', 'fixed-length', 'feature', 'vectors', 'projected', 'into', 'a', 'low-dimensional', 'space', '(', 'i-vectors', ')', 'rather', 'than', 'audio', 'recordings', '.'], 'entities': [{'type': 'Material', 'start': 4, 'end': 6}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'OtherScientificTerm', 'start': 16, 'end': 19}, {'type': 'OtherScientificTerm', 'start': 22, 'end': 27}, {'type': 'Material', 'start': 29, 'end': 31}], 'relations': [{'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 3, 'tail': 1}, {'type': 'Compare', 'head': 4, 'tail': 3}], 'orig_id': '1644e1ee-6a58-4183-962c-2f06fb7dc42e'}\n",
      "952 {'tokens': ['For', 'one', 'thing', ',', 'learning', 'methodology', 'applicable', 'in', 'general', 'domains', 'does', 'not', 'readily', 'lend', 'itself', 'in', 'the', 'linguistic', 'domain', '.'], 'entities': [{'type': 'Method', 'start': 4, 'end': 6}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'Material', 'start': 17, 'end': 19}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': '960d4a66-476e-434b-9fa7-e8f2d5b76255'}\n",
      "963 {'tokens': ['Although', 'the', 'system', 'performs', 'well', 'within', 'a', 'limited', 'textual', 'domain', ',', 'further', 'research', 'is', 'needed', 'to', 'make', 'it', 'effective', 'for', 'open-domain', 'question', 'answering', 'and', 'text', 'summarisation', '.'], 'entities': [{'type': 'Generic', 'start': 2, 'end': 3}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'Generic', 'start': 17, 'end': 18}, {'type': 'Task', 'start': 20, 'end': 23}, {'type': 'Task', 'start': 24, 'end': 26}], 'relations': [{'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 4}, {'type': 'Conjunction', 'head': 3, 'tail': 4}], 'orig_id': 'd9c16902-0d79-4fa2-94a6-5396a6b94ad3'}\n",
      "966 {'tokens': ['Our', 'experiments', 'also', 'show', 'that', 'current', 'technology', 'for', 'extracting', 'subcategorization', 'frames', 'initially', 'designed', 'for', 'written', 'texts', 'works', 'equally', 'well', 'for', 'spoken', 'language', '.'], 'entities': [{'type': 'Generic', 'start': 6, 'end': 7}, {'type': 'Task', 'start': 8, 'end': 11}, {'type': 'Material', 'start': 14, 'end': 16}, {'type': 'Material', 'start': 20, 'end': 22}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 0, 'tail': 3}, {'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': '97dd7585-a723-43a9-9218-e6b7c3077c85'}\n",
      "973 {'tokens': ['Experimental', 'results', 'show', 'that', 'our', 'approach', 'improves', 'domain-specific', 'word', 'alignment', 'in', 'terms', 'of', 'both', 'precision', 'and', 'recall', ',', 'achieving', 'a', 'relative', 'error', 'rate', 'reduction', 'of', '6.56', '%', 'as', 'compared', 'with', 'the', 'state-of-the-art', 'technologies', '.'], 'entities': [{'type': 'Generic', 'start': 5, 'end': 6}, {'type': 'Task', 'start': 7, 'end': 10}, {'type': 'Metric', 'start': 14, 'end': 15}, {'type': 'Metric', 'start': 16, 'end': 17}, {'type': 'Metric', 'start': 20, 'end': 24}, {'type': 'Generic', 'start': 31, 'end': 33}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 5}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Conjunction', 'head': 2, 'tail': 3}, {'type': 'Evaluate-for', 'head': 3, 'tail': 0}, {'type': 'Evaluate-for', 'head': 4, 'tail': 0}, {'type': 'Evaluate-for', 'head': 4, 'tail': 5}], 'orig_id': '1028ae8f-64c8-4c70-ae1d-20c1f3d4a393'}\n",
      "1030 {'tokens': ['While', 'conditional', 'independencies', 'are', 'well', 'studied', 'and', 'frequently', 'used', 'in', 'causal', 'induction', 'algorithms', ',', 'Verma', 'constraints', 'are', 'still', 'poorly', 'understood', ',', 'and', 'rarely', 'applied', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 1, 'end': 3}, {'type': 'Method', 'start': 10, 'end': 13}, {'type': 'OtherScientificTerm', 'start': 14, 'end': 16}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 2, 'tail': 0}], 'orig_id': '1e0e541b-f673-4ee9-b418-204e9a514309'}\n",
      "1135 {'tokens': ['We', 'demonstrate', 'that', 'our', 'models', 'outperform', 'the', 'state-of-the-art', 'on', 'ultra-wide', 'baseline', 'matching', 'and', 'approach', 'human', 'accuracy', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Task', 'start': 9, 'end': 12}, {'type': 'Metric', 'start': 14, 'end': 16}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Evaluate-for', 'head': 2, 'tail': 1}], 'orig_id': 'a8b521ab-9d35-457b-b255-1364f33fdb20'}\n",
      "1135 {'tokens': ['We', 'demonstrate', 'that', 'our', 'models', 'outperform', 'the', 'state-of-the-art', 'on', 'ultra-wide', 'baseline', 'matching', 'and', 'approach', 'human', 'accuracy', '.'], 'entities': [{'type': 'Generic', 'start': 4, 'end': 5}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Task', 'start': 9, 'end': 12}, {'type': 'Metric', 'start': 14, 'end': 16}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Evaluate-for', 'head': 2, 'tail': 1}], 'orig_id': 'a8b521ab-9d35-457b-b255-1364f33fdb20'}\n",
      "1192 {'tokens': ['To', 'validate', 'our', 'method', ',', 'we', 'compare', 'it', 'with', 'the', 'Maximum', 'Likelihood', '(', 'ML', ')', 'estimation', 'method', 'under', 'sparse', 'data', 'and', 'with', 'the', 'Expectation', 'Maximization', '(', 'EM', ')', 'algorithm', 'under', 'incomplete', 'data', 'respectively', '.'], 'entities': [{'type': 'Generic', 'start': 3, 'end': 4}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Method', 'start': 10, 'end': 17}, {'type': 'Material', 'start': 18, 'end': 20}, {'type': 'Method', 'start': 23, 'end': 29}, {'type': 'Material', 'start': 30, 'end': 32}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 1, 'tail': 4}, {'type': 'Used-for', 'head': 3, 'tail': 1}, {'type': 'Used-for', 'head': 3, 'tail': 2}, {'type': 'Used-for', 'head': 5, 'tail': 1}, {'type': 'Used-for', 'head': 5, 'tail': 4}], 'orig_id': '66c18097-1f98-4184-8dcd-cbacfcd89962'}\n",
      "1192 {'tokens': ['To', 'validate', 'our', 'method', ',', 'we', 'compare', 'it', 'with', 'the', 'Maximum', 'Likelihood', '(', 'ML', ')', 'estimation', 'method', 'under', 'sparse', 'data', 'and', 'with', 'the', 'Expectation', 'Maximization', '(', 'EM', ')', 'algorithm', 'under', 'incomplete', 'data', 'respectively', '.'], 'entities': [{'type': 'Generic', 'start': 3, 'end': 4}, {'type': 'Generic', 'start': 7, 'end': 8}, {'type': 'Method', 'start': 10, 'end': 17}, {'type': 'Material', 'start': 18, 'end': 20}, {'type': 'Method', 'start': 23, 'end': 29}, {'type': 'Material', 'start': 30, 'end': 32}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 1, 'tail': 4}, {'type': 'Used-for', 'head': 3, 'tail': 1}, {'type': 'Used-for', 'head': 3, 'tail': 2}, {'type': 'Used-for', 'head': 5, 'tail': 1}, {'type': 'Used-for', 'head': 5, 'tail': 4}], 'orig_id': '66c18097-1f98-4184-8dcd-cbacfcd89962'}\n",
      "1232 {'tokens': ['The', 'method', 'outperforms', 'its', 'state-of-the-art', 'counterparts', 'in', 'both', 'accuracy', 'and', 'scalability', ',', 'especially', 'when', 'it', 'comes', 'to', 'the', 'retrieval', 'of', 'small', ',', 'rotated', 'objects', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 2}, {'type': 'Generic', 'start': 5, 'end': 6}, {'type': 'Metric', 'start': 8, 'end': 9}, {'type': 'Metric', 'start': 10, 'end': 11}, {'type': 'Task', 'start': 18, 'end': 24}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 4}, {'type': 'Compare', 'head': 1, 'tail': 0}, {'type': 'Evaluate-for', 'head': 2, 'tail': 0}, {'type': 'Evaluate-for', 'head': 2, 'tail': 1}, {'type': 'Evaluate-for', 'head': 3, 'tail': 0}, {'type': 'Evaluate-for', 'head': 3, 'tail': 1}], 'orig_id': 'dbfc5985-fc35-47b6-8e43-342494ce6a86'}\n",
      "1246 {'tokens': ['When', 'used', 'as', 'pre-training', 'for', 'action', 'recognition', ',', 'our', 'method', 'gives', 'significant', 'gains', 'over', 'learning', 'without', 'external', 'data', 'on', 'benchmark', 'datasets', 'like', 'UCF101', 'and', 'HMDB51', '.'], 'entities': [{'type': 'Method', 'start': 3, 'end': 4}, {'type': 'Task', 'start': 5, 'end': 7}, {'type': 'Method', 'start': 8, 'end': 10}, {'type': 'Method', 'start': 14, 'end': 18}, {'type': 'Generic', 'start': 19, 'end': 21}, {'type': 'Material', 'start': 22, 'end': 23}, {'type': 'Material', 'start': 24, 'end': 25}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 2, 'tail': 0}, {'type': 'Compare', 'head': 2, 'tail': 3}, {'type': 'Evaluate-for', 'head': 4, 'tail': 2}, {'type': 'Evaluate-for', 'head': 4, 'tail': 3}, {'type': 'Hyponym-of', 'head': 5, 'tail': 4}, {'type': 'Conjunction', 'head': 5, 'tail': 6}, {'type': 'Hyponym-of', 'head': 6, 'tail': 4}], 'orig_id': '6a0e4a72-e4a4-43a2-a972-b0e8bf555f8b'}\n",
      "1302 {'tokens': ['This', 'mining', 'procedure', 'of', 'AND', 'and', 'OR', 'patterns', 'is', 'readily', 'integrated', 'to', 'boosting', ',', 'which', 'improves', 'the', 'generalization', 'ability', 'over', 'the', 'conventional', 'boosting', 'decision', 'trees', 'and', 'boosting', 'decision', 'stumps', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 3}, {'type': 'OtherScientificTerm', 'start': 4, 'end': 8}, {'type': 'Method', 'start': 12, 'end': 13}, {'type': 'Metric', 'start': 17, 'end': 19}, {'type': 'Method', 'start': 22, 'end': 25}, {'type': 'Method', 'start': 26, 'end': 29}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Part-of', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 4}, {'type': 'Compare', 'head': 2, 'tail': 5}, {'type': 'Evaluate-for', 'head': 3, 'tail': 2}, {'type': 'Evaluate-for', 'head': 3, 'tail': 4}, {'type': 'Evaluate-for', 'head': 3, 'tail': 5}, {'type': 'Conjunction', 'head': 4, 'tail': 5}], 'orig_id': 'ff317dca-b529-42c7-8cfa-bd730a784298'}\n",
      "1302 {'tokens': ['This', 'mining', 'procedure', 'of', 'AND', 'and', 'OR', 'patterns', 'is', 'readily', 'integrated', 'to', 'boosting', ',', 'which', 'improves', 'the', 'generalization', 'ability', 'over', 'the', 'conventional', 'boosting', 'decision', 'trees', 'and', 'boosting', 'decision', 'stumps', '.'], 'entities': [{'type': 'Generic', 'start': 1, 'end': 3}, {'type': 'OtherScientificTerm', 'start': 4, 'end': 8}, {'type': 'Method', 'start': 12, 'end': 13}, {'type': 'Metric', 'start': 17, 'end': 19}, {'type': 'Method', 'start': 22, 'end': 25}, {'type': 'Method', 'start': 26, 'end': 29}], 'relations': [{'type': 'Used-for', 'head': 0, 'tail': 1}, {'type': 'Part-of', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 4}, {'type': 'Compare', 'head': 2, 'tail': 5}, {'type': 'Evaluate-for', 'head': 3, 'tail': 2}, {'type': 'Evaluate-for', 'head': 3, 'tail': 4}, {'type': 'Evaluate-for', 'head': 3, 'tail': 5}, {'type': 'Conjunction', 'head': 4, 'tail': 5}], 'orig_id': 'ff317dca-b529-42c7-8cfa-bd730a784298'}\n",
      "1324 {'tokens': ['In', 'opposition', 'to', 'the', 'approach', 'of', 'Kamp', 'and', 'Rohrer', 'the', 'exact', 'meaning', 'of', 'the', 'tenses', 'is', 'fixed', 'by', 'the', 'resolution', 'component', 'and', 'not', 'in', 'the', 'process', 'of', 'syntactic', 'analysis', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 11, 'end': 15}, {'type': 'Method', 'start': 19, 'end': 21}, {'type': 'Method', 'start': 27, 'end': 29}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': 'ac750a35-86b7-45c4-acf4-62c852d122e8'}\n",
      "1345 {'tokens': ['Unification', 'is', 'often', 'the', 'appropriate', 'method', 'for', 'expressing', 'relations', 'between', 'representations', 'in', 'the', 'form', 'of', 'feature', 'structures', ';', 'however', ',', 'there', 'are', 'circumstances', 'in', 'which', 'a', 'different', 'approach', 'is', 'desirable', '.'], 'entities': [{'type': 'Method', 'start': 0, 'end': 1}, {'type': 'Generic', 'start': 5, 'end': 6}, {'type': 'Task', 'start': 8, 'end': 11}, {'type': 'OtherScientificTerm', 'start': 15, 'end': 17}, {'type': 'Generic', 'start': 27, 'end': 28}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 3, 'tail': 2}, {'type': 'Compare', 'head': 4, 'tail': 1}], 'orig_id': 'abd21387-3d9b-4717-b304-eea5ce4a1581'}\n",
      "1377 {'tokens': ['This', 'paper', 'presents', 'an', 'entirely', 'data-driven', 'model', 'selection', 'procedure', 'based', 'on', 'genetic', 'search', ',', 'which', 'is', 'shown', 'to', 'outperform', 'both', 'knowledge-based', 'and', 'random', 'selection', 'procedures', 'on', 'two', 'different', 'language', 'modeling', 'tasks', '(', 'Arabic', 'and', 'Turkish', ')', '.'], 'entities': [{'type': 'Method', 'start': 4, 'end': 9}, {'type': 'Method', 'start': 11, 'end': 13}, {'type': 'Method', 'start': 20, 'end': 25}, {'type': 'Task', 'start': 28, 'end': 31}, {'type': 'Material', 'start': 32, 'end': 33}, {'type': 'Material', 'start': 34, 'end': 35}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Hyponym-of', 'head': 4, 'tail': 3}, {'type': 'Conjunction', 'head': 4, 'tail': 5}, {'type': 'Hyponym-of', 'head': 5, 'tail': 3}], 'orig_id': 'fac7dd7c-3bb3-4ab5-b52f-ab1d9c80f3f9'}\n",
      "1492 {'tokens': ['The', 'issue', 'of', 'system', 'response', 'to', 'users', 'has', 'been', 'extensively', 'studied', 'by', 'the', 'natural', 'language', 'generation', 'community', ',', 'though', 'rarely', 'in', 'the', 'context', 'of', 'dialog', 'systems', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 3, 'end': 5}, {'type': 'Task', 'start': 13, 'end': 17}, {'type': 'Task', 'start': 24, 'end': 26}], 'relations': [{'type': 'Part-of', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': 'd93778aa-de6b-48e8-b30f-dc59eb6e09ae'}\n",
      "1512 {'tokens': ['Sentence', 'ambiguities', 'can', 'be', 'resolved', 'by', 'using', 'domain', 'targeted', 'preference', 'knowledge', 'without', 'using', 'complicated', 'large', 'knowledgebases', '.'], 'entities': [{'type': 'OtherScientificTerm', 'start': 0, 'end': 2}, {'type': 'OtherScientificTerm', 'start': 7, 'end': 11}, {'type': 'Material', 'start': 15, 'end': 16}], 'relations': [{'type': 'Used-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 2}], 'orig_id': '1d1e9af2-a848-4c1c-a180-e8980f9f6a03'}\n",
      "1559 {'tokens': ['Instead', 'of', 'building', 'individual', 'classifiers', 'per', 'ambiguous', 'wordform', ',', 'we', 'introduce', 'a', 'lemma-based', 'approach', '.'], 'entities': [{'type': 'Method', 'start': 4, 'end': 5}, {'type': 'OtherScientificTerm', 'start': 6, 'end': 8}, {'type': 'Method', 'start': 12, 'end': 14}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Used-for', 'head': 1, 'tail': 0}], 'orig_id': 'fb68910c-9f13-40e4-aa53-cb5035cec26a'}\n",
      "1586 {'tokens': ['Specifically', ',', 'this', 'system', 'is', 'designed', 'to', 'deterministically', 'choose', 'between', 'pronominalization', ',', 'superordinate', 'substitution', ',', 'and', 'definite', 'noun', 'phrase', 'reiteration', '.'], 'entities': [{'type': 'Generic', 'start': 3, 'end': 4}, {'type': 'OtherScientificTerm', 'start': 10, 'end': 11}, {'type': 'OtherScientificTerm', 'start': 12, 'end': 14}, {'type': 'OtherScientificTerm', 'start': 16, 'end': 20}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': '2f167cc3-c0b6-4012-a59f-35de1a20b8ff'}\n",
      "1586 {'tokens': ['Specifically', ',', 'this', 'system', 'is', 'designed', 'to', 'deterministically', 'choose', 'between', 'pronominalization', ',', 'superordinate', 'substitution', ',', 'and', 'definite', 'noun', 'phrase', 'reiteration', '.'], 'entities': [{'type': 'Generic', 'start': 3, 'end': 4}, {'type': 'OtherScientificTerm', 'start': 10, 'end': 11}, {'type': 'OtherScientificTerm', 'start': 12, 'end': 14}, {'type': 'OtherScientificTerm', 'start': 16, 'end': 20}], 'relations': [{'type': 'Compare', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 3}], 'orig_id': '2f167cc3-c0b6-4012-a59f-35de1a20b8ff'}\n",
      "1665 {'tokens': ['While', 'this', 'task', 'has', 'much', 'in', 'common', 'with', 'paraphrases', 'acquisition', 'which', 'aims', 'to', 'discover', 'semantic', 'equivalence', 'between', 'verbs', ',', 'the', 'main', 'challenge', 'of', 'entailment', 'acquisition', 'is', 'to', 'capture', 'asymmetric', ',', 'or', 'directional', ',', 'relations', '.'], 'entities': [{'type': 'Generic', 'start': 2, 'end': 3}, {'type': 'Task', 'start': 8, 'end': 10}, {'type': 'OtherScientificTerm', 'start': 14, 'end': 16}, {'type': 'Task', 'start': 23, 'end': 25}, {'type': 'OtherScientificTerm', 'start': 28, 'end': 34}], 'relations': [{'type': 'Compare', 'head': 0, 'tail': 1}, {'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Used-for', 'head': 3, 'tail': 4}], 'orig_id': '3c536228-4eb1-40bc-adc5-8d3f0d159d75'}\n",
      "1736 {'tokens': ['We', 'evaluate', 'the', 'models', 'on', 'standard', 'test', 'sets', ',', 'showing', 'performance', 'competitive', 'with', 'existing', 'methods', 'trained', 'on', 'hand', 'prepared', 'datasets', '.'], 'entities': [{'type': 'Generic', 'start': 3, 'end': 4}, {'type': 'Generic', 'start': 6, 'end': 8}, {'type': 'Generic', 'start': 14, 'end': 15}, {'type': 'Material', 'start': 17, 'end': 20}], 'relations': [{'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Evaluate-for', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 2, 'tail': 0}, {'type': 'Used-for', 'head': 3, 'tail': 2}], 'orig_id': 'f027e9af-9bdc-49eb-83c9-e7bfe348618c'}\n",
      "1766 {'tokens': ['Later', ',', 'however', ',', 'Breiman', 'cast', 'serious', 'doubt', 'on', 'this', 'explanation', 'by', 'introducing', 'a', 'boosting', 'algorithm', ',', 'arc-gv', ',', 'that', 'can', 'generate', 'a', 'higher', 'margins', 'distribution', 'than', 'AdaBoost', 'and', 'yet', 'performs', 'worse', '.'], 'entities': [{'type': 'Method', 'start': 14, 'end': 16}, {'type': 'Method', 'start': 17, 'end': 18}, {'type': 'OtherScientificTerm', 'start': 24, 'end': 26}, {'type': 'Method', 'start': 27, 'end': 28}], 'relations': [{'type': 'Hyponym-of', 'head': 1, 'tail': 0}, {'type': 'Used-for', 'head': 1, 'tail': 2}, {'type': 'Compare', 'head': 1, 'tail': 3}], 'orig_id': '09fdbb3b-f675-43cb-b9f4-f5ed4d6d7403'}\n",
      "1770 {'tokens': ['The', 'transfer', 'phase', 'in', 'machine', 'translation', '(', 'MT', ')', 'systems', 'has', 'been', 'considered', 'to', 'be', 'more', 'complicated', 'than', 'analysis', 'and', 'generation', ',', 'since', 'it', 'is', 'inherently', 'a', 'conglomeration', 'of', 'individual', 'lexical', 'rules', '.'], 'entities': [{'type': 'Task', 'start': 1, 'end': 3}, {'type': 'Task', 'start': 4, 'end': 10}, {'type': 'Task', 'start': 18, 'end': 19}, {'type': 'Task', 'start': 20, 'end': 21}, {'type': 'Generic', 'start': 23, 'end': 24}, {'type': 'OtherScientificTerm', 'start': 30, 'end': 32}], 'relations': [{'type': 'Part-of', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Conjunction', 'head': 2, 'tail': 3}], 'orig_id': 'aab6c169-a43d-4828-975a-6a441fffacff'}\n",
      "1770 {'tokens': ['The', 'transfer', 'phase', 'in', 'machine', 'translation', '(', 'MT', ')', 'systems', 'has', 'been', 'considered', 'to', 'be', 'more', 'complicated', 'than', 'analysis', 'and', 'generation', ',', 'since', 'it', 'is', 'inherently', 'a', 'conglomeration', 'of', 'individual', 'lexical', 'rules', '.'], 'entities': [{'type': 'Task', 'start': 1, 'end': 3}, {'type': 'Task', 'start': 4, 'end': 10}, {'type': 'Task', 'start': 18, 'end': 19}, {'type': 'Task', 'start': 20, 'end': 21}, {'type': 'Generic', 'start': 23, 'end': 24}, {'type': 'OtherScientificTerm', 'start': 30, 'end': 32}], 'relations': [{'type': 'Part-of', 'head': 0, 'tail': 1}, {'type': 'Compare', 'head': 0, 'tail': 2}, {'type': 'Compare', 'head': 0, 'tail': 3}, {'type': 'Conjunction', 'head': 2, 'tail': 3}], 'orig_id': 'aab6c169-a43d-4828-975a-6a441fffacff'}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Compare']:\n",
    "            head = rel['head']\n",
    "            tail = rel['tail']\n",
    "            flag = True\n",
    "            for j, a_rel in enumerate(row['relations']):\n",
    "                if i != j and head == a_rel['head'] and a_rel['type'] not in ['Evaluate-for']:\n",
    "                    if not rel_exist(tail, a_rel['tail'], a_rel['type'], row['relations']):\n",
    "                        flag = False\n",
    "                        \n",
    "                if i != j and head == a_rel['tail'] and a_rel['type'] not in ['Evaluate-for']:\n",
    "                    if not rel_exist(a_rel['head'], tail, a_rel['type'], row['relations']):\n",
    "                        flag = False\n",
    "                        \n",
    "                if i != j and tail == a_rel['head'] and a_rel['type'] not in ['Evaluate-for']:\n",
    "                    if not rel_exist(head, a_rel['tail'], a_rel['type'], row['relations']):\n",
    "                        flag = False\n",
    "                \n",
    "                if i != j and tail == a_rel['tail'] and a_rel['type'] not in ['Evaluate-for']:\n",
    "                    if not rel_exist(a_rel['head'], head, a_rel['type'], row['relations']):\n",
    "                        flag = False\n",
    "            if not flag:\n",
    "                print(z, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state-of-the-art', 'methods']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['Extensive', 'experiments', 'on', 'two', 'tasks', 'have', 'demonstrated', 'the', 'superiority', 'of', 'our', 'method', 'over', 'the', 'state-of-the-art', 'methods', '.']\n",
    "x[14:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with(rels, prefix=['Compare']):\n",
    "    for rel in rels:\n",
    "        if rel.startswith(tuple(prefix)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    relations.append(\n",
    "        [f'{rel[\"type\"]}({rel[\"head\"]},{rel[\"tail\"]})' for rel in row['relations']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Compare(0,1)', 'Used-for(0,2)', 'Used-for(1,2)', 'Hyponym-of(4,3)', 'Feature-of(5,4)', 'Conjunction(5,6)', 'Feature-of(6,4)']\n",
      "1 ['Conjunction(2,1)', 'Used-for(4,3)', 'Conjunction(4,5)', 'Used-for(5,3)', 'Conjunction(6,2)', 'Compare(7,8)']\n",
      "2 ['Used-for(1,2)', 'Compare(4,5)', 'Used-for(4,6)', 'Used-for(5,6)']\n",
      "3 ['Compare(0,1)', 'Used-for(0,2)', 'Used-for(1,2)', 'Used-for(4,3)']\n",
      "4 ['Compare(0,1)', 'Compare(0,2)', 'Conjunction(1,2)']\n",
      "5 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Evaluate-for(1,2)']\n",
      "6 ['Compare(0,1)', 'Used-for(0,2)', 'Used-for(1,2)']\n",
      "7 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Evaluate-for(1,2)', 'Used-for(3,0)', 'Used-for(3,2)']\n",
      "8 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Compare(2,3)']\n",
      "9 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)', 'Evaluate-for(3,0)', 'Evaluate-for(3,2)']\n",
      "10 ['Evaluate-for(0,1)', 'Used-for(1,2)', 'Compare(2,3)']\n",
      "11 ['Compare(0,1)', 'Feature-of(2,1)', 'Feature-of(3,2)']\n",
      "12 ['Used-for(1,0)', 'Compare(1,3)', 'Evaluate-for(2,1)', 'Evaluate-for(2,3)']\n",
      "13 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "14 ['Used-for(0,1)', 'Compare(1,3)', 'Used-for(2,3)']\n",
      "15 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(1,2)']\n",
      "16 ['Compare(0,1)', 'Used-for(1,2)', 'Used-for(3,1)']\n",
      "17 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(1,2)', 'Used-for(2,3)']\n",
      "18 ['Used-for(0,1)', 'Compare(0,2)', 'Used-for(2,3)']\n",
      "19 ['Compare(0,1)', 'Compare(0,2)', 'Conjunction(1,2)']\n",
      "20 ['Compare(0,3)', 'Feature-of(1,0)', 'Feature-of(4,3)']\n",
      "21 ['Compare(0,2)', 'Evaluate-for(3,0)', 'Evaluate-for(3,2)']\n",
      "22 ['Evaluate-for(0,1)', 'Evaluate-for(0,3)', 'Compare(1,3)', 'Evaluate-for(2,1)', 'Evaluate-for(2,3)', 'Used-for(4,3)']\n",
      "23 ['Used-for(0,1)', 'Used-for(0,2)', 'Compare(1,2)']\n",
      "24 ['Used-for(0,2)', 'Compare(2,3)', 'Used-for(3,4)']\n",
      "25 ['Compare(0,1)', 'Used-for(2,1)', 'Evaluate-for(3,0)', 'Evaluate-for(3,1)']\n",
      "26 ['Used-for(0,2)', 'Compare(1,0)', 'Used-for(1,2)', 'Hyponym-of(3,2)', 'Hyponym-of(4,2)', 'Conjunction(4,3)']\n",
      "27 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(2,1)']\n",
      "28 ['Hyponym-of(0,1)', 'Part-of(2,3)', 'Compare(2,4)']\n",
      "29 ['Used-for(1,2)', 'Compare(1,3)', 'Used-for(4,3)', 'Conjunction(4,5)', 'Used-for(5,3)']\n",
      "30 ['Compare(0,3)', 'Evaluate-for(1,0)', 'Conjunction(1,2)', 'Evaluate-for(1,3)', 'Evaluate-for(2,0)', 'Evaluate-for(2,3)']\n",
      "31 ['Used-for(0,2)', 'Compare(0,3)', 'Used-for(1,0)', 'Used-for(3,2)']\n",
      "32 ['Compare(0,1)', 'Compare(0,2)', 'Conjunction(1,2)']\n",
      "33 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Evaluate-for(1,2)', 'Used-for(3,2)', 'Conjunction(3,4)', 'Used-for(4,2)', 'Conjunction(4,5)', 'Used-for(5,2)', 'Conjunction(5,6)', 'Used-for(6,2)']\n",
      "34 ['Used-for(0,1)', 'Compare(0,3)', 'Used-for(2,0)']\n",
      "35 ['Used-for(1,0)', 'Used-for(2,0)', 'Compare(2,1)']\n",
      "36 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Evaluate-for(1,2)']\n",
      "37 ['Used-for(1,0)', 'Compare(1,2)', 'Feature-of(1,3)', 'Feature-of(2,3)']\n",
      "38 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "39 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)', 'Evaluate-for(3,0)', 'Evaluate-for(3,1)']\n",
      "40 ['Used-for(1,0)', 'Part-of(2,1)', 'Compare(2,3)', 'Used-for(2,4)', 'Used-for(3,4)']\n",
      "41 ['Hyponym-of(1,0)', 'Compare(1,2)', 'Hyponym-of(2,0)', 'Used-for(3,4)']\n",
      "42 ['Evaluate-for(0,1)', 'Compare(1,2)', 'Used-for(1,3)', 'Used-for(2,3)']\n",
      "43 ['Compare(0,1)', 'Compare(0,3)', 'Used-for(2,1)', 'Used-for(4,3)']\n",
      "44 ['Used-for(0,1)', 'Used-for(1,2)', 'Compare(4,5)']\n",
      "45 ['Compare(0,2)', 'Used-for(1,0)', 'Used-for(2,3)']\n",
      "46 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "47 ['Compare(0,2)', 'Evaluate-for(1,0)', 'Evaluate-for(1,2)']\n",
      "48 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "49 ['Used-for(0,1)', 'Used-for(0,2)', 'Evaluate-for(3,4)', 'Evaluate-for(3,5)', 'Compare(4,5)']\n",
      "50 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(1,2)']\n",
      "51 ['Compare(0,3)', 'Evaluate-for(1,0)', 'Evaluate-for(1,3)', 'Evaluate-for(2,0)', 'Used-for(2,3)']\n",
      "52 ['Used-for(2,3)', 'Used-for(3,1)', 'Compare(4,3)']\n",
      "53 ['Evaluate-for(0,2)', 'Evaluate-for(0,3)', 'Feature-of(1,0)', 'Compare(3,2)']\n",
      "54 ['Evaluate-for(1,0)', 'Compare(1,3)', 'Used-for(2,3)', 'Used-for(2,4)', 'Conjunction(3,4)']\n",
      "55 ['Used-for(0,1)', 'Used-for(0,3)', 'Used-for(1,2)', 'Compare(2,3)']\n",
      "56 ['Used-for(0,1)', 'Compare(0,5)', 'Evaluate-for(2,0)', 'Conjunction(2,3)', 'Evaluate-for(3,0)', 'Evaluate-for(4,0)', 'Evaluate-for(4,5)']\n",
      "57 ['Used-for(0,1)', 'Used-for(1,2)', 'Compare(3,4)']\n",
      "58 ['Compare(0,1)', 'Used-for(0,2)', 'Used-for(1,2)', 'Evaluate-for(2,3)']\n",
      "59 ['Compare(1,0)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "60 ['Compare(0,1)', 'Used-for(0,2)', 'Used-for(1,2)', 'Used-for(3,4)', 'Used-for(5,3)']\n",
      "61 ['Used-for(1,0)', 'Compare(1,2)', 'Used-for(2,0)', 'Evaluate-for(3,2)', 'Used-for(4,5)']\n",
      "62 ['Compare(0,1)', 'Compare(0,3)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "63 ['Compare(1,2)', 'Compare(1,4)', 'Used-for(3,1)', 'Used-for(3,2)', 'Used-for(5,1)', 'Used-for(5,4)']\n",
      "64 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(1,2)']\n",
      "65 ['Used-for(0,4)', 'Compare(1,0)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)', 'Evaluate-for(3,0)', 'Evaluate-for(3,1)']\n",
      "66 ['Used-for(0,1)', 'Used-for(2,0)', 'Compare(2,3)', 'Evaluate-for(4,2)', 'Evaluate-for(4,3)', 'Hyponym-of(5,4)', 'Conjunction(5,6)', 'Hyponym-of(6,4)']\n",
      "67 ['Used-for(0,1)', 'Part-of(1,2)', 'Compare(2,4)', 'Compare(2,5)', 'Evaluate-for(3,2)', 'Evaluate-for(3,4)', 'Evaluate-for(3,5)', 'Conjunction(4,5)']\n",
      "68 ['Used-for(1,2)', 'Used-for(3,2)', 'Compare(4,1)']\n",
      "69 ['Evaluate-for(0,1)', 'Evaluate-for(0,2)', 'Compare(1,2)']\n",
      "70 ['Compare(0,2)', 'Used-for(1,0)', 'Used-for(2,3)', 'Hyponym-of(4,3)', 'Conjunction(4,5)', 'Hyponym-of(5,3)']\n",
      "71 ['Evaluate-for(1,0)', 'Evaluate-for(2,3)', 'Compare(3,0)']\n",
      "72 ['Hyponym-of(1,0)', 'Compare(2,3)', 'Part-of(4,5)']\n",
      "73 ['Evaluate-for(0,2)', 'Evaluate-for(0,4)', 'Evaluate-for(0,5)', 'Used-for(2,1)', 'Compare(4,5)']\n",
      "74 ['Evaluate-for(0,2)', 'Evaluate-for(0,3)', 'Evaluate-for(1,2)', 'Evaluate-for(1,3)', 'Compare(2,3)']\n",
      "75 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "76 ['Compare(1,2)', 'Evaluate-for(3,1)', 'Evaluate-for(3,2)']\n",
      "77 ['Hyponym-of(2,1)', 'Compare(3,4)', 'Evaluate-for(5,3)']\n",
      "78 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "79 ['Compare(0,1)', 'Used-for(1,2)', 'Used-for(3,4)']\n",
      "80 ['Compare(0,1)', 'Evaluate-for(2,0)', 'Evaluate-for(2,1)']\n",
      "81 ['Evaluate-for(1,0)', 'Evaluate-for(1,2)', 'Compare(2,0)', 'Used-for(3,2)']\n",
      "82 ['Hyponym-of(1,0)', 'Used-for(1,2)', 'Compare(1,3)']\n",
      "83 ['Part-of(0,1)', 'Compare(0,2)', 'Compare(0,3)', 'Conjunction(2,3)']\n",
      "84 ['Used-for(0,1)', 'Used-for(1,2)', 'Compare(3,5)', 'Evaluate-for(4,3)', 'Evaluate-for(4,5)']\n",
      "85 ['Compare(0,5)', 'Evaluate-for(1,0)', 'Evaluate-for(1,5)', 'Hyponym-of(2,1)', 'Conjunction(2,3)', 'Hyponym-of(3,1)', 'Conjunction(3,4)', 'Hyponym-of(4,1)']\n"
     ]
    }
   ],
   "source": [
    "relations = [r for r in relations if len(r) >= 3 and starts_with(r)]\n",
    "for i, rels in enumerate(relations):\n",
    "    print(i, rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 {'tokens': ['Although', 'the', 'system', 'performs', 'well', 'within', 'a', 'limited', 'textual', 'domain', ',', 'further', 'research', 'is', 'needed', 'to', 'make', 'it', 'effective', 'for', 'open-domain', 'question', 'answering', 'and', 'text', 'summarisation', '.'], 'entities': [{'type': 'Generic', 'start': 2, 'end': 3}, {'type': 'Material', 'start': 8, 'end': 10}, {'type': 'Generic', 'start': 17, 'end': 18}, {'type': 'Task', 'start': 20, 'end': 23}, {'type': 'Task', 'start': 24, 'end': 26}], 'relations': [{'type': 'Evaluate-for', 'head': 1, 'tail': 0}, {'type': 'Compare', 'head': 1, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 3}, {'type': 'Used-for', 'head': 2, 'tail': 4}, {'type': 'Conjunction', 'head': 3, 'tail': 4}], 'orig_id': 'd9c16902-0d79-4fa2-94a6-5396a6b94ad3'}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pair = []\n",
    "for z, row in enumerate(data):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        if rel['type'] in ['Compare']:\n",
    "            flag = False\n",
    "                    \n",
    "            hs, he, ht = entities[rel['head']]['start'], entities[rel['head']]['end'], entities[rel['head']]['type']\n",
    "            ts, te, tt = entities[rel['tail']]['start'], entities[rel['tail']]['end'], entities[rel['tail']]['type']\n",
    "            \n",
    "            if (ht not in ['Generic', 'OtherScientificTerm'] and \n",
    "                tt not in ['Generic', 'OtherScientificTerm'] and tt != ht):\n",
    "                count += 1\n",
    "                pair.append((ht, tt))\n",
    "                print(z, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/thoang/workspace/spert/data/datasets/conll04/conll04_train.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    tokens = row['tokens']\n",
    "    for ent in row['entities']:\n",
    "        s, e, t = ent['start'], ent['end'], ent['type']\n",
    "        if t != 'Other' and not t.istitle():\n",
    "            print(' '.join(tokens[s:e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/thoang/workspace/spert/data/datasets/ade/ade_full.json', 'r') as f:\n",
    "    ade = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "pari_type = defaultdict(Counter)\n",
    "for z, row in enumerate(ade):\n",
    "    entities = row['entities']\n",
    "    for i, rel in enumerate(row['relations']):\n",
    "        pari_type[rel['type']][(entities[rel['head']]['type'], \n",
    "                                    entities[rel['tail']]['type'])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'Adverse-Effect': Counter({('Adverse-Effect', 'Drug'): 6821})})"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pari_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (9,11)\n",
      "head\n",
      "--------------\n",
      "(3,4) inside (3,5)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (4,6)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(15,16) inside (15,17)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (8,12)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (9,13)\n",
      "head\n",
      "--------------\n",
      "(19,20) inside (19,22)\n",
      "head\n",
      "--------------\n",
      "(7,8) inside (7,11)\n",
      "head\n",
      "--------------\n",
      "(6,7) inside (6,13)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (9,11)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (8,10)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (4,6)\n",
      "head\n",
      "--------------\n",
      "(20,21) inside (20,22)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(12,13) inside (12,14)\n",
      "head\n",
      "--------------\n",
      "(13,14) inside (13,15)\n",
      "head\n",
      "--------------\n",
      "(12,13) inside (12,14)\n",
      "head\n",
      "--------------\n",
      "(14,15) inside (14,16)\n",
      "head\n",
      "--------------\n",
      "(3,4) inside (3,5)\n",
      "head\n",
      "--------------\n",
      "(16,17) inside (14,18)\n",
      "else\n",
      "['lead']\n",
      "['high', 'blood', 'lead', 'level']\n",
      "--------------\n",
      "(32,33) inside (32,36)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (8,10)\n",
      "head\n",
      "--------------\n",
      "(42,43) inside (42,44)\n",
      "head\n",
      "--------------\n",
      "(12,13) inside (9,14)\n",
      "else\n",
      "['5-FU']\n",
      "['dermatologic', 'and', 'ocular', '5-FU', 'toxicities']\n",
      "--------------\n",
      "(23,24) inside (23,25)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (2,6)\n",
      "else\n",
      "['LTG']\n",
      "['elevated', 'serum', 'LTG', 'levels']\n",
      "--------------\n",
      "(24,25) inside (24,26)\n",
      "head\n",
      "--------------\n",
      "(2,3) inside (2,4)\n",
      "head\n",
      "--------------\n",
      "(15,16) inside (12,17)\n",
      "else\n",
      "['vincristine']\n",
      "['life', '-', 'threatening', 'vincristine', 'neurotoxicity']\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(2,3) inside (1,4)\n",
      "else\n",
      "['valproate']\n",
      "['fetal', 'valproate', 'syndrome']\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(12,13) inside (12,14)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (7,10)\n",
      "else\n",
      "['chloroquine']\n",
      "['severe', 'chloroquine', 'toxicity']\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (0,3)\n",
      "else\n",
      "['ethambutol']\n",
      "['Ocular', 'ethambutol', 'toxicity']\n",
      "--------------\n",
      "(15,16) inside (15,17)\n",
      "head\n",
      "--------------\n",
      "(17,18) inside (16,19)\n",
      "else\n",
      "['DGTX']\n",
      "['cardiac', 'DGTX', 'toxicity']\n",
      "--------------\n",
      "(14,15) inside (10,15)\n",
      "tail\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(13,14) inside (12,16)\n",
      "else\n",
      "['infliximab']\n",
      "['severe', 'infliximab', 'infusion', 'reaction']\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(23,24) inside (23,25)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(7,8) inside (7,9)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (4,6)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (0,3)\n",
      "else\n",
      "['digoxin']\n",
      "['Fatal', 'digoxin', 'poisoning']\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(21,22) inside (21,23)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (0,3)\n",
      "else\n",
      "['gold']\n",
      "['Pulmonary', 'gold', 'toxicity']\n",
      "--------------\n",
      "(17,18) inside (17,19)\n",
      "head\n",
      "--------------\n",
      "(7,9) inside (6,11)\n",
      "else\n",
      "['perhexiline', 'maleate']\n",
      "['fatal', 'perhexiline', 'maleate', 'liver', 'injury']\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(23,24) inside (23,25)\n",
      "head\n",
      "--------------\n",
      "(28,29) inside (28,30)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (8,10)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (9,11)\n",
      "head\n",
      "--------------\n",
      "(20,21) inside (18,21)\n",
      "tail\n",
      "--------------\n",
      "(23,24) inside (23,25)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (4,9)\n",
      "else\n",
      "['lithium']\n",
      "['serum', 'lithium', 'level', 'had', 'increased']\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(11,12) inside (11,13)\n",
      "head\n",
      "--------------\n",
      "(9,10) inside (9,12)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (4,7)\n",
      "head\n",
      "--------------\n",
      "(7,8) inside (7,9)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(4,5) inside (4,6)\n",
      "head\n",
      "--------------\n",
      "(46,47) inside (42,47)\n",
      "tail\n",
      "--------------\n",
      "(9,10) inside (9,11)\n",
      "head\n",
      "--------------\n",
      "(11,12) inside (11,13)\n",
      "head\n",
      "--------------\n",
      "(17,18) inside (17,19)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (1,3)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (1,3)\n",
      "head\n",
      "--------------\n",
      "(11,12) inside (9,13)\n",
      "else\n",
      "['5-FU']\n",
      "['severe', 'acute', '5-FU', 'reactions']\n",
      "--------------\n",
      "(31,33) inside (31,34)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(1,2) inside (1,3)\n",
      "head\n",
      "--------------\n",
      "(15,16) inside (15,17)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(21,22) inside (21,23)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(17,18) inside (17,19)\n",
      "head\n",
      "--------------\n",
      "(10,11) inside (8,11)\n",
      "tail\n",
      "--------------\n",
      "(5,6) inside (5,7)\n",
      "head\n",
      "--------------\n",
      "(18,19) inside (18,20)\n",
      "head\n",
      "--------------\n",
      "(6,7) inside (6,8)\n",
      "head\n",
      "--------------\n",
      "(18,19) inside (18,20)\n",
      "head\n",
      "--------------\n",
      "(2,3) inside (2,4)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(3,4) inside (3,5)\n",
      "head\n",
      "--------------\n",
      "(6,7) inside (6,8)\n",
      "head\n",
      "--------------\n",
      "(7,8) inside (7,9)\n",
      "head\n",
      "--------------\n",
      "(27,28) inside (24,30)\n",
      "else\n",
      "['lamotrigine']\n",
      "['doubling', 'of', 'the', 'lamotrigine', 'blood', 'level']\n",
      "--------------\n",
      "(0,1) inside (0,2)\n",
      "head\n",
      "--------------\n",
      "(0,1) inside (0,3)\n",
      "head\n",
      "--------------\n",
      "(7,8) inside (7,10)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (5,10)\n",
      "else\n",
      "['theophylline']\n",
      "['increase', 'in', 'the', 'theophylline', 'level']\n",
      "--------------\n",
      "(4,5) inside (4,12)\n",
      "head\n",
      "--------------\n",
      "(5,6) inside (2,7)\n",
      "else\n",
      "['theophylline']\n",
      "['increase', 'in', 'the', 'theophylline', 'level']\n",
      "--------------\n",
      "(8,9) inside (8,10)\n",
      "head\n",
      "--------------\n",
      "(25,26) inside (23,27)\n",
      "else\n",
      "['tacrolimus']\n",
      "['interfere', 'with', 'tacrolimus', 'metabolism']\n",
      "--------------\n",
      "(7,8) inside (4,8)\n",
      "tail\n",
      "--------------\n",
      "(17,19) inside (16,20)\n",
      "else\n",
      "['vitamin', 'D']\n",
      "['acute', 'vitamin', 'D', 'poisoning']\n",
      "--------------\n",
      "(12,14) inside (11,15)\n",
      "else\n",
      "['vitamin', 'D']\n",
      "['acute', 'vitamin', 'D', 'poisoning']\n",
      "--------------\n",
      "(9,11) inside (9,12)\n",
      "head\n",
      "--------------\n",
      "(14,16) inside (14,17)\n",
      "head\n",
      "--------------\n",
      "(8,9) inside (8,10)\n",
      "head\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "match_head = []\n",
    "match_tail = []\n",
    "match_else = []\n",
    "for row in ade:\n",
    "    for i, ent in enumerate(row['entities']):\n",
    "        for j, a_ent in enumerate(row['entities']):\n",
    "            if i != j:\n",
    "                if a_ent['start'] >= ent['start'] and a_ent['end'] <= ent['end']:\n",
    "                    flag = False\n",
    "                    print(f\"({a_ent['start']},{a_ent['end']}) inside ({ent['start']},{ent['end']})\")\n",
    "                    \n",
    "                    if a_ent['start'] == ent['start']:\n",
    "                        print('head')\n",
    "                        match_head.append((ent['type'], a_ent['type']))\n",
    "                    elif a_ent['end'] == ent['end']:\n",
    "                        print('tail')\n",
    "                        match_tail.append((ent['type'], a_ent['type']))\n",
    "                    else:\n",
    "                        print('else')\n",
    "                        match_else.append((ent['type'], a_ent['type']))\n",
    "                        print(row['tokens'][a_ent['start']:a_ent['end']])\n",
    "                        print(row['tokens'][ent['start']:ent['end']])\n",
    "                    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in ade:\n",
    "#     for i, ent in enumerate(row['entities']):\n",
    "#         for j, a_ent in enumerate(row['entities']):\n",
    "#             if i != j:\n",
    "#                 if ent['start'] < a_ent['start'] and ent['end'] > a_ent['start'] and ent['end'] < a_ent['end']:\n",
    "#                     print(f\"({a_ent['start']},{a_ent['end']}) inside ({ent['start']},{ent['end']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (spert)",
   "language": "python",
   "name": "pycharm-5e67594c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
